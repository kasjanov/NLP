{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19d53da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corus\n",
    "import matplotlib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyconll\n",
    "import ru_core_news_sm\n",
    "import spacy\n",
    "import tensorflow\n",
    "import warnings\n",
    "\n",
    "from corus import load_ne5\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import DefaultTagger, RegexpTagger, UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from razdel import tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
    "from spacy import displacy\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D, Input\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, GRU, LSTM, Dropout\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b7c77cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71b300",
   "metadata": {},
   "source": [
    "## POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "98da4eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\xiaomi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\xiaomi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\xiaomi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\xiaomi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\xiaomi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "# Требуется для токенизации\n",
    "nltk.download('punkt')\n",
    "# Требуется для parts of speech tagging\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ac0aafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset('RB')\n",
    "nltk.help.upenn_tagset('NN')\n",
    "nltk.help.upenn_tagset('VB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3bd97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    (r'.*ет$', 'VB'),                \n",
    "    (r'.*ал$', 'VBD'),                 \n",
    "    (r'.*ешь$', 'VB'),                 \n",
    "    (r'.*ое$', 'NN'),                  \n",
    "    (r'^-?[0-9]+(\\.[0-9]+)?$', 'CD'),  \n",
    "    (r'.*ая', 'NN'), \n",
    "    (r'.*', 'NN'),                      # nouns (default) \n",
    "    (r'.*ий$', 'NN'),                 \n",
    "    (r'.*ую$', 'NN')                   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f3a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('ru_syntagrus-ud-train-a.conllu')\n",
    "full_test = pyconll.load_from_file('ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b91b4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета NOUN\n",
      ". PUNCT\n",
      "Начальник NOUN\n",
      "областного ADJ\n",
      "управления NOUN\n",
      "связи NOUN\n",
      "Семен PROPN\n",
      "Еремеевич PROPN\n",
      "был AUX\n",
      "человек NOUN\n",
      "простой ADJ\n",
      ", PUNCT\n",
      "приходил VERB\n",
      "на ADP\n",
      "работу NOUN\n",
      "всегда ADV\n",
      "вовремя ADV\n",
      ", PUNCT\n",
      "здоровался VERB\n",
      "с ADP\n",
      "секретаршей NOUN\n",
      "за ADP\n",
      "руку NOUN\n",
      "и CCONJ\n",
      "иногда ADV\n",
      "даже PART\n",
      "писал VERB\n",
      "в ADP\n",
      "стенгазету NOUN\n",
      "заметки NOUN\n",
      "под ADP\n",
      "псевдонимом NOUN\n",
      "\" PUNCT\n",
      "Муха NOUN\n",
      "\" PUNCT\n",
      ". PUNCT\n"
     ]
    }
   ],
   "source": [
    "for sent in full_train[:2]:\n",
    "    for token in sent:\n",
    "        print(token.form, token.upos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f15f316f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c343ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24516, 8906, 8906)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fdata_train), len(fdata_test), len(fdata_sent_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96d3e5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DefaultTagger\n",
    "D_TAG = DefaultTagger(fdata_train)\n",
    "accuracy_D_TAG = D_TAG.evaluate(fdata_test)\n",
    "\n",
    "# R_TAG = RegexpTagger(patterns)\n",
    "# R_TAG.evaluate(fdata_test)\n",
    "\n",
    "# UnigramTagger\n",
    "U_TAG = UnigramTagger(fdata_train)\n",
    "accuracy_U_TAG = U_TAG.evaluate(fdata_test)\n",
    "\n",
    "# BigramTagger\n",
    "Bi_TAG = BigramTagger(fdata_train)\n",
    "accuracy_Bi_TAG = Bi_TAG.evaluate(fdata_test)\n",
    "\n",
    "# TrigramTagger\n",
    "T_TAG = TrigramTagger(fdata_train)\n",
    "accuracy_T_TAG = T_TAG.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5984871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy Default tagger=0.000\n",
      "accuracy Unigram tagger=0.824\n",
      "accuracy Bigram tagger=0.609\n",
      "accuracy Trigram tagger=0.178\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy Default tagger={accuracy_D_TAG:.3f}')\n",
    "print(f'accuracy Unigram tagger={accuracy_U_TAG:.3f}')\n",
    "print(f'accuracy Bigram tagger={accuracy_Bi_TAG:.3f}')\n",
    "print(f'accuracy Trigram tagger={accuracy_T_TAG:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe878f07",
   "metadata": {},
   "source": [
    "#### Комбинация тэггеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63750b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backoff_tagger(train_sents, tagger_classes, backoff=None):\n",
    "    for cls in tagger_classes:\n",
    "        backoff = cls(train_sents, backoff=backoff)\n",
    "    return backoff\n",
    "\n",
    "\n",
    "backoff = DefaultTagger('NN') \n",
    "Combi_TAG = backoff_tagger(fdata_train,  \n",
    "                     [UnigramTagger, BigramTagger, TrigramTagger],  \n",
    "                     backoff = backoff) \n",
    "  \n",
    "accuracy_Combi_TAG = Combi_TAG.evaluate(fdata_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "71f3db2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy Combi tagger=0.828\n"
     ]
    }
   ],
   "source": [
    "print(f'accuracy Combi tagger={accuracy_Combi_TAG:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1e85a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "115a82b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)\n",
    "test_enc_labels = le.transform(test_label)\n",
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d54a01e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizers = [CountVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
    "               TfidfVectorizer(ngram_range=(1, 3), analyzer='char'), \n",
    "               HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=1000)] \n",
    "vectorizers_word = [CountVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
    "               TfidfVectorizer(ngram_range=(1, 3), analyzer='word'), \n",
    "               HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=1000)] \n",
    "n_features = [2000, 3000, 5000, 10000]\n",
    "hvectorizer = [HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=feat) for feat in n_features]\n",
    "hvectorizer_word = [HashingVectorizer(ngram_range=(1, 3), analyzer='word', n_features=feat) for feat in n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ff2620e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer='char', ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.92      0.91      0.92     11247\n",
      "         ADP       0.98      1.00      0.99     10255\n",
      "         ADV       0.92      0.90      0.91      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.98      0.93      4276\n",
      "         DET       0.88      0.75      0.81      2978\n",
      "        INTJ       0.36      0.36      0.36        11\n",
      "        NOUN       0.92      0.95      0.94     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.85      0.91      0.88      1436\n",
      "        PART       0.95      0.78      0.86      3762\n",
      "        PRON       0.83      0.89      0.86      5346\n",
      "       PROPN       0.79      0.59      0.67      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.91      0.86      2176\n",
      "         SYM       1.00      0.68      0.81        53\n",
      "        VERB       0.94      0.94      0.94     12617\n",
      "           X       0.48      0.15      0.23       105\n",
      "\n",
      "    accuracy                           0.93    115000\n",
      "   macro avg       0.85      0.82      0.83    115000\n",
      "weighted avg       0.93      0.93      0.93    115000\n",
      "\n",
      "TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.90      0.91      0.91     11247\n",
      "         ADP       0.99      0.99      0.99     10255\n",
      "         ADV       0.92      0.87      0.89      5986\n",
      "         AUX       0.81      0.97      0.89      1058\n",
      "       CCONJ       0.88      0.98      0.93      4276\n",
      "         DET       0.80      0.83      0.82      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.90      0.96      0.93     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.85      0.90      0.87      1436\n",
      "        PART       0.95      0.79      0.86      3762\n",
      "        PRON       0.87      0.84      0.86      5346\n",
      "       PROPN       0.80      0.52      0.63      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.91      0.86      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.93      0.93      0.93     12617\n",
      "           X       0.45      0.09      0.14       105\n",
      "\n",
      "    accuracy                           0.92    115000\n",
      "   macro avg       0.83      0.78      0.79    115000\n",
      "weighted avg       0.92      0.92      0.92    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=1000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.84      0.84      0.84     11247\n",
      "         ADP       0.97      0.98      0.98     10255\n",
      "         ADV       0.83      0.79      0.81      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.80      0.80      0.80      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.84      0.90      0.87     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.81      0.82      0.82      1436\n",
      "        PART       0.92      0.78      0.84      3762\n",
      "        PRON       0.84      0.86      0.85      5346\n",
      "       PROPN       0.72      0.45      0.55      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.88      0.84      0.86     12617\n",
      "           X       0.31      0.05      0.08       105\n",
      "\n",
      "    accuracy                           0.89    115000\n",
      "   macro avg       0.79      0.75      0.76    115000\n",
      "weighted avg       0.88      0.89      0.88    115000\n",
      "\n",
      "CountVectorizer(ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.95      0.40      0.57     11247\n",
      "         ADP       0.99      0.48      0.65     10255\n",
      "         ADV       0.93      0.78      0.85      5986\n",
      "         AUX       0.83      0.76      0.79      1058\n",
      "       CCONJ       0.90      0.20      0.33      4276\n",
      "         DET       0.84      0.66      0.74      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.98      0.68      0.80     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.88      0.52      0.65      1436\n",
      "        PART       0.97      0.75      0.84      3762\n",
      "        PRON       0.83      0.80      0.82      5346\n",
      "       PROPN       0.95      0.13      0.23      4315\n",
      "       PUNCT       0.37      1.00      0.54     21941\n",
      "       SCONJ       0.72      0.84      0.77      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.95      0.43      0.59     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.64    115000\n",
      "   macro avg       0.67      0.47      0.51    115000\n",
      "weighted avg       0.83      0.64      0.65    115000\n",
      "\n",
      "TfidfVectorizer(ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.95      0.44      0.60     11247\n",
      "         ADP       0.99      0.48      0.65     10255\n",
      "         ADV       0.96      0.78      0.86      5986\n",
      "         AUX       0.83      0.88      0.85      1058\n",
      "       CCONJ       0.89      0.20      0.33      4276\n",
      "         DET       0.90      0.64      0.75      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.98      0.68      0.80     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.89      0.55      0.68      1436\n",
      "        PART       0.97      0.75      0.84      3762\n",
      "        PRON       0.80      0.86      0.83      5346\n",
      "       PROPN       0.93      0.16      0.27      4315\n",
      "       PUNCT       0.38      1.00      0.55     21941\n",
      "       SCONJ       0.79      0.85      0.82      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.97      0.46      0.62     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.65    115000\n",
      "   macro avg       0.68      0.48      0.52    115000\n",
      "weighted avg       0.84      0.65      0.66    115000\n",
      "\n",
      "HashingVectorizer(n_features=1000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.43      0.21      0.28     11247\n",
      "         ADP       0.83      0.47      0.60     10255\n",
      "         ADV       0.59      0.63      0.61      5986\n",
      "         AUX       0.70      0.94      0.80      1058\n",
      "       CCONJ       0.84      0.18      0.30      4276\n",
      "         DET       0.49      0.55      0.52      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.25      0.53      0.34     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.41      0.43      0.42      1436\n",
      "        PART       0.86      0.76      0.81      3762\n",
      "        PRON       0.64      0.76      0.70      5346\n",
      "       PROPN       0.32      0.08      0.13      4315\n",
      "       PUNCT       0.00      0.00      0.00     21941\n",
      "       SCONJ       0.67      0.95      0.78      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.45      0.25      0.32     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.37    115000\n",
      "   macro avg       0.42      0.38      0.37    115000\n",
      "weighted avg       0.39      0.37      0.35    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=2000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.87      0.87     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.87      0.82      0.84      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.84      0.75      0.80      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.86      0.93      0.89     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.83      0.84      0.83      1436\n",
      "        PART       0.94      0.78      0.85      3762\n",
      "        PRON       0.83      0.89      0.85      5346\n",
      "       PROPN       0.74      0.41      0.53      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.89      0.88      0.89     12617\n",
      "           X       0.38      0.06      0.10       105\n",
      "\n",
      "    accuracy                           0.90    115000\n",
      "   macro avg       0.81      0.76      0.77    115000\n",
      "weighted avg       0.90      0.90      0.90    115000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HashingVectorizer(analyzer='char', n_features=3000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.87      0.88      0.87     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.87      0.82      0.85      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.89      0.97      0.93      4276\n",
      "         DET       0.87      0.72      0.79      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.87      0.93      0.90     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.83      0.83      0.83      1436\n",
      "        PART       0.94      0.78      0.85      3762\n",
      "        PRON       0.82      0.91      0.86      5346\n",
      "       PROPN       0.79      0.38      0.51      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.79      0.88        53\n",
      "        VERB       0.87      0.90      0.89     12617\n",
      "           X       0.22      0.13      0.17       105\n",
      "\n",
      "    accuracy                           0.90    115000\n",
      "   macro avg       0.80      0.77      0.78    115000\n",
      "weighted avg       0.90      0.90      0.90    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=5000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.88      0.88     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.90      0.83      0.86      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.83      0.78      0.81      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.87      0.94      0.90     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.84      0.87      0.85      1436\n",
      "        PART       0.94      0.78      0.85      3762\n",
      "        PRON       0.83      0.86      0.85      5346\n",
      "       PROPN       0.76      0.41      0.54      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.81      0.90      0.85      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.89      0.90      0.89     12617\n",
      "           X       0.50      0.05      0.09       105\n",
      "\n",
      "    accuracy                           0.90    115000\n",
      "   macro avg       0.82      0.77      0.77    115000\n",
      "weighted avg       0.90      0.90      0.90    115000\n",
      "\n",
      "HashingVectorizer(analyzer='char', n_features=10000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.88      0.88      0.88     11247\n",
      "         ADP       0.98      0.99      0.98     10255\n",
      "         ADV       0.90      0.84      0.87      5986\n",
      "         AUX       0.81      0.97      0.88      1058\n",
      "       CCONJ       0.88      0.97      0.93      4276\n",
      "         DET       0.83      0.78      0.81      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.88      0.94      0.91     27241\n",
      "      NO_TAG       1.00      1.00      1.00       197\n",
      "         NUM       0.84      0.88      0.86      1436\n",
      "        PART       0.94      0.79      0.85      3762\n",
      "        PRON       0.82      0.87      0.85      5346\n",
      "       PROPN       0.78      0.42      0.54      4315\n",
      "       PUNCT       1.00      1.00      1.00     21941\n",
      "       SCONJ       0.82      0.86      0.84      2176\n",
      "         SYM       1.00      0.64      0.78        53\n",
      "        VERB       0.90      0.90      0.90     12617\n",
      "           X       0.71      0.05      0.09       105\n",
      "\n",
      "    accuracy                           0.91    115000\n",
      "   macro avg       0.83      0.77      0.78    115000\n",
      "weighted avg       0.91      0.91      0.90    115000\n",
      "\n",
      "HashingVectorizer(n_features=2000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.50      0.26      0.34     11247\n",
      "         ADP       0.90      0.48      0.62     10255\n",
      "         ADV       0.68      0.69      0.68      5986\n",
      "         AUX       0.75      0.94      0.84      1058\n",
      "       CCONJ       0.89      0.18      0.31      4276\n",
      "         DET       0.67      0.53      0.59      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.61      0.58      0.59     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.53      0.47      0.50      1436\n",
      "        PART       0.91      0.76      0.83      3762\n",
      "        PRON       0.69      0.85      0.76      5346\n",
      "       PROPN       0.39      0.10      0.15      4315\n",
      "       PUNCT       0.48      1.00      0.65     21941\n",
      "       SCONJ       0.76      0.90      0.82      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.55      0.31      0.39     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.58    115000\n",
      "   macro avg       0.52      0.45      0.45    115000\n",
      "weighted avg       0.62      0.58      0.56    115000\n",
      "\n",
      "HashingVectorizer(n_features=3000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.55      0.30      0.39     11247\n",
      "         ADP       0.92      0.48      0.63     10255\n",
      "         ADV       0.77      0.71      0.74      5986\n",
      "         AUX       0.77      0.94      0.85      1058\n",
      "       CCONJ       0.93      0.18      0.30      4276\n",
      "         DET       0.71      0.57      0.63      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.65      0.60      0.62     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.61      0.49      0.54      1436\n",
      "        PART       0.90      0.78      0.84      3762\n",
      "        PRON       0.74      0.83      0.79      5346\n",
      "       PROPN       0.44      0.12      0.18      4315\n",
      "       PUNCT       0.47      1.00      0.64     21941\n",
      "       SCONJ       0.73      0.95      0.83      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.60      0.34      0.43     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.60    115000\n",
      "   macro avg       0.54      0.46      0.47    115000\n",
      "weighted avg       0.65      0.60      0.58    115000\n",
      "\n",
      "HashingVectorizer(n_features=5000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.63      0.33      0.44     11247\n",
      "         ADP       0.91      0.48      0.63     10255\n",
      "         ADV       0.82      0.72      0.77      5986\n",
      "         AUX       0.79      0.95      0.86      1058\n",
      "       CCONJ       0.92      0.19      0.31      4276\n",
      "         DET       0.69      0.70      0.70      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.70      0.62      0.65     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.66      0.48      0.55      1436\n",
      "        PART       0.91      0.76      0.83      3762\n",
      "        PRON       0.79      0.79      0.79      5346\n",
      "       PROPN       0.54      0.13      0.21      4315\n",
      "       PUNCT       0.45      1.00      0.62     21941\n",
      "       SCONJ       0.76      0.88      0.82      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.67      0.36      0.47     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.61    115000\n",
      "   macro avg       0.57      0.47      0.48    115000\n",
      "weighted avg       0.68      0.61      0.59    115000\n",
      "\n",
      "HashingVectorizer(n_features=10000, ngram_range=(1, 3))\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADJ       0.72      0.36      0.48     11247\n",
      "         ADP       0.96      0.48      0.64     10255\n",
      "         ADV       0.88      0.75      0.81      5986\n",
      "         AUX       0.81      0.88      0.84      1058\n",
      "       CCONJ       0.88      0.20      0.33      4276\n",
      "         DET       0.72      0.75      0.74      2978\n",
      "        INTJ       0.00      0.00      0.00        11\n",
      "        NOUN       0.77      0.64      0.70     27241\n",
      "      NO_TAG       0.00      0.00      0.00       197\n",
      "         NUM       0.74      0.58      0.65      1436\n",
      "        PART       0.96      0.75      0.84      3762\n",
      "        PRON       0.83      0.78      0.81      5346\n",
      "       PROPN       0.67      0.15      0.24      4315\n",
      "       PUNCT       0.42      1.00      0.59     21941\n",
      "       SCONJ       0.78      0.88      0.83      2176\n",
      "         SYM       0.00      0.00      0.00        53\n",
      "        VERB       0.75      0.41      0.53     12617\n",
      "           X       0.00      0.00      0.00       105\n",
      "\n",
      "    accuracy                           0.63    115000\n",
      "   macro avg       0.60      0.48      0.50    115000\n",
      "weighted avg       0.72      0.63      0.62    115000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f1_scores = []\n",
    "acc_scores = []\n",
    "\n",
    "for vectorizer in vectorizers + vectorizers_word + hvectorizer + hvectorizer_word:\n",
    "    X_train = vectorizer.fit_transform(train_tok)\n",
    "    X_test = vectorizer.transform(test_tok[:115000])\n",
    "    \n",
    "    lr = LogisticRegression(random_state=0, max_iter=100)\n",
    "    lr.fit(X_train, train_enc_labels)\n",
    "    pred = lr.predict(X_test)\n",
    "    f1 = f1_score(test_enc_labels[:115000], pred, average='weighted')\n",
    "    f1_scores.append(f1)\n",
    "    acc = accuracy_score(test_enc_labels[:115000], pred)\n",
    "    acc_scores.append(acc)\n",
    "    \n",
    "    print(vectorizer)\n",
    "    print(classification_report(test_enc_labels[:115000], pred, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b740ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>f1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.927971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.921185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.903654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.901192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.896959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.895273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.882215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.662748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.650922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HashingVectorizer(n_features=10000, ngram_rang...</td>\n",
       "      <td>0.620016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
       "      <td>0.594828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
       "      <td>0.577227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
       "      <td>0.555762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
       "      <td>0.345039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Vectorizer  f1_score\n",
       "0   CountVectorizer(analyzer='char', ngram_range=(...  0.927971\n",
       "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.921185\n",
       "9   HashingVectorizer(analyzer='char', n_features=...  0.903654\n",
       "8   HashingVectorizer(analyzer='char', n_features=...  0.901192\n",
       "7   HashingVectorizer(analyzer='char', n_features=...  0.896959\n",
       "6   HashingVectorizer(analyzer='char', n_features=...  0.895273\n",
       "2   HashingVectorizer(analyzer='char', n_features=...  0.882215\n",
       "4                 TfidfVectorizer(ngram_range=(1, 3))  0.662748\n",
       "3                 CountVectorizer(ngram_range=(1, 3))  0.650922\n",
       "13  HashingVectorizer(n_features=10000, ngram_rang...  0.620016\n",
       "12  HashingVectorizer(n_features=5000, ngram_range...  0.594828\n",
       "11  HashingVectorizer(n_features=3000, ngram_range...  0.577227\n",
       "10  HashingVectorizer(n_features=2000, ngram_range...  0.555762\n",
       "5   HashingVectorizer(n_features=1000, ngram_range...  0.345039"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + hvectorizer + hvectorizer_word,\n",
    "                            'f1_score': f1_scores})\n",
    "result_model.sort_values('f1_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "12f111ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CountVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.929643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TfidfVectorizer(analyzer='char', ngram_range=(...</td>\n",
       "      <td>0.923609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.907417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.904974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.901243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.898939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HashingVectorizer(analyzer='char', n_features=...</td>\n",
       "      <td>0.885157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TfidfVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.652930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CountVectorizer(ngram_range=(1, 3))</td>\n",
       "      <td>0.642148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>HashingVectorizer(n_features=10000, ngram_rang...</td>\n",
       "      <td>0.629365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>HashingVectorizer(n_features=5000, ngram_range...</td>\n",
       "      <td>0.612913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HashingVectorizer(n_features=3000, ngram_range...</td>\n",
       "      <td>0.601226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HashingVectorizer(n_features=2000, ngram_range...</td>\n",
       "      <td>0.584009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HashingVectorizer(n_features=1000, ngram_range...</td>\n",
       "      <td>0.365243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Vectorizer  Accuracy\n",
       "0   CountVectorizer(analyzer='char', ngram_range=(...  0.929643\n",
       "1   TfidfVectorizer(analyzer='char', ngram_range=(...  0.923609\n",
       "9   HashingVectorizer(analyzer='char', n_features=...  0.907417\n",
       "8   HashingVectorizer(analyzer='char', n_features=...  0.904974\n",
       "7   HashingVectorizer(analyzer='char', n_features=...  0.901243\n",
       "6   HashingVectorizer(analyzer='char', n_features=...  0.898939\n",
       "2   HashingVectorizer(analyzer='char', n_features=...  0.885157\n",
       "4                 TfidfVectorizer(ngram_range=(1, 3))  0.652930\n",
       "3                 CountVectorizer(ngram_range=(1, 3))  0.642148\n",
       "13  HashingVectorizer(n_features=10000, ngram_rang...  0.629365\n",
       "12  HashingVectorizer(n_features=5000, ngram_range...  0.612913\n",
       "11  HashingVectorizer(n_features=3000, ngram_range...  0.601226\n",
       "10  HashingVectorizer(n_features=2000, ngram_range...  0.584009\n",
       "5   HashingVectorizer(n_features=1000, ngram_range...  0.365243"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model_acc = pd.DataFrame({'Vectorizer': vectorizers + vectorizers_word + hvectorizer + hvectorizer_word,\n",
    "                            'Accuracy': acc_scores})\n",
    "result_model_acc.sort_values('Accuracy', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866a122",
   "metadata": {},
   "source": [
    "#### Вывод: лучшую метрику показали символьные N-граммы на CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a807eb87",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf0d71",
   "metadata": {},
   "source": [
    "#### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4325ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.ann', '001.txt', '002.ann', '002.txt', '003.ann', '003.txt', '004.ann', '004.txt', '005.ann', '005.txt', '006.ann', '006.txt', '007.ann', '007.txt', '008.ann', '008.txt', '009.ann', '009.txt', '010.ann', '010.txt', '011.ann', '011.txt', '012.ann', '012.txt', '013.ann', '013.txt', '014.ann', '014.txt', '015 (!).ann', '015 (!).txt', '016.ann', '016.txt', '017.ann', '017.txt', '018.ann', '018.txt', '019.ann', '019.txt', '020.ann', '020.txt', '021.ann', '021.txt', '022.ann', '022.txt', '023.ann', '023.txt', '025.ann', '025.txt', '026.ann', '026.txt', '027.ann', '027.txt', '028.ann', '028.txt', '029.ann', '029.txt', '030.ann', '030.txt', '031.ann', '031.txt', '032.ann', '032.txt', '033.ann', '033.txt', '034.ann', '034.txt', '035.ann', '035.txt', '036.ann', '036.txt', '037.ann', '037.txt', '038.ann', '038.txt', '039.ann', '039.txt', '03_12_12a.ann', '03_12_12a.txt', '03_12_12b.ann', '03_12_12b.txt', '03_12_12c.ann', '03_12_12c.txt', '03_12_12d.ann', '03_12_12d.txt', '03_12_12g.ann', '03_12_12g.txt', '03_12_12h.ann', '03_12_12h.txt', '040.ann', '040.txt', '041.ann', '041.txt', '042.ann', '042.txt', '043.ann', '043.txt', '044.ann', '044.txt', '045.ann', '045.txt', '046.ann', '046.txt', '047.ann', '047.txt', '048.ann', '048.txt', '049.ann', '049.txt', '04_02_13a_abdulatipov.ann', '04_02_13a_abdulatipov.txt', '04_03_13a_sorokin.ann', '04_03_13a_sorokin.txt', '04_12_12b.ann', '04_12_12b.txt', '04_12_12d.ann', '04_12_12d.txt', '04_12_12f.ann', '04_12_12f.txt', '04_12_12g.ann', '04_12_12g.txt', '04_12_12h_corr.ann', '04_12_12h_corr.txt', '050.ann', '050.txt', '051.ann', '051.txt', '052.ann', '052.txt', '053.ann', '053.txt', '054.ann', '054.txt', '055.ann', '055.txt', '056.ann', '056.txt', '057.ann', '057.txt', '058.ann', '058.txt', '059.ann', '059.txt', '060.ann', '060.txt', '061.ann', '061.txt', '062.ann', '062.txt', '063.ann', '063.txt', '064.ann', '064.txt', '065.ann', '065.txt', '066.ann', '066.txt', '067.ann', '067.txt', '068.ann', '068.txt', '069.ann', '069.txt', '070.ann', '070.txt', '071.ann', '071.txt', '072.ann', '072.txt', '073.ann', '073.txt', '074.ann', '074.txt', '075.ann', '075.txt', '076.ann', '076.txt', '077.ann', '077.txt', '078.ann', '078.txt', '079.ann', '079.txt', '080.ann', '080.txt', '081.ann', '081.txt', '082.ann', '082.txt', '083.ann', '083.txt', '084.ann', '084.txt', '085.ann', '085.txt', '086.ann', '086.txt', '087.ann', '087.txt', '088.ann', '088.txt', '089.ann', '089.txt', '090.ann', '090.txt', '091.ann', '091.txt', '092.ann', '092.txt', '093.ann', '093.txt', '094.ann', '094.txt', '095.ann', '095.txt', '096.ann', '096.txt', '097.ann', '097.txt', '098.ann', '098.txt', '099.ann', '099.txt', '09_01_13.ann', '09_01_13.txt', '09_01_13a.ann', '09_01_13a.txt', '09_01_13c.ann', '09_01_13c.txt', '09_01_13d.ann', '09_01_13d.txt', '09_01_13e.ann', '09_01_13e.txt', '09_01_13h.ann', '09_01_13h.txt', '09_01_13i.ann', '09_01_13i.txt', '100.ann', '100.txt', '1000.ann', '1000.txt', '1001.ann', '1001.txt', '1002.ann', '1002.txt', '1003.ann', '1003.txt', '1004.ann', '1004.txt', '1005.ann', '1005.txt', '1006.ann', '1006.txt', '1007.ann', '1007.txt', '1008.ann', '1008.txt', '1009.ann', '1009.txt', '101.ann', '101.txt', '1010.ann', '1010.txt', '1011.ann', '1011.txt', '1012.ann', '1012.txt', '1013.ann', '1013.txt', '1014.ann', '1014.txt', '1015.ann', '1015.txt', '1016.ann', '1016.txt', '1017.ann', '1017.txt', '1018.ann', '1018.txt', '1019.ann', '1019.txt', '102.ann', '102.txt', '1020.ann', '1020.txt', '1021.ann', '1021.txt', '1022.ann', '1022.txt', '1023.ann', '1023.txt', '1024.ann', '1024.txt', '1025.ann', '1025.txt', '1026.ann', '1026.txt', '1027.ann', '1027.txt', '1028.ann', '1028.txt', '1029.ann', '1029.txt', '103.ann', '103.txt', '1030.ann', '1030.txt', '1031.ann', '1031.txt', '1032.ann', '1032.txt', '1033.ann', '1033.txt', '1034.ann', '1034.txt', '1035.ann', '1035.txt', '1036.ann', '1036.txt', '1037.ann', '1037.txt', '1038.ann', '1038.txt', '1039.ann', '1039.txt', '104.ann', '104.txt', '1040.ann', '1040.txt', '1041.ann', '1041.txt', '1042.ann', '1042.txt', '1043.ann', '1043.txt', '1044.ann', '1044.txt', '1045.ann', '1045.txt', '1046.ann', '1046.txt', '1047.ann', '1047.txt', '1048.ann', '1048.txt', '1049.ann', '1049.txt', '105.ann', '105.txt', '1050.ann', '1050.txt', '106.ann', '106.txt', '107.ann', '107.txt', '108.ann', '108.txt', '109.ann', '109.txt', '10_01_13a.ann', '10_01_13a.txt', '10_01_13d.ann', '10_01_13d.txt', '10_01_13i.ann', '10_01_13i.txt', '110.ann', '110.txt', '1100.ann', '1100.txt', '1101.ann', '1101.txt', '1102.ann', '1102.txt', '1103.ann', '1103.txt', '1104.ann', '1104.txt', '1105.ann', '1105.txt', '1106.ann', '1106.txt', '1107.ann', '1107.txt', '1108.ann', '1108.txt', '1109.ann', '1109.txt', '111.ann', '111.txt', '1110.ann', '1110.txt', '1111.ann', '1111.txt', '1112.ann', '1112.txt', '1113.ann', '1113.txt', '1114.ann', '1114.txt', '1115.ann', '1115.txt', '1116.ann', '1116.txt', '1117.ann', '1117.txt', '1118.ann', '1118.txt', '1119.ann', '1119.txt', '112.ann', '112.txt', '1120.ann', '1120.txt', '1121.ann', '1121.txt', '1122.ann', '1122.txt', '1123.ann', '1123.txt', '1124.ann', '1124.txt', '1125.ann', '1125.txt', '1126.ann', '1126.txt', '1127.ann', '1127.txt', '1128.ann', '1128.txt', '113.ann', '113.txt', '1130.ann', '1130.txt', '1131.ann', '1131.txt', '1132.ann', '1132.txt', '1133.ann', '1133.txt', '1134.ann', '1134.txt', '1135.ann', '1135.txt', '1136.ann', '1136.txt', '1137.ann', '1137.txt', '1138.ann', '1138.txt', '1139.ann', '1139.txt', '114.ann', '114.txt', '1140.ann', '1140.txt', '1141.ann', '1141.txt', '1142.ann', '1142.txt', '1143.ann', '1143.txt', '1144.ann', '1144.txt', '1145.ann', '1145.txt', '1146.ann', '1146.txt', '1147.ann', '1147.txt', '1148.ann', '1148.txt', '1149.ann', '1149.txt', '115.ann', '115.txt', '1150.ann', '1150.txt', '1151.ann', '1151.txt', '1152.ann', '1152.txt', '1153.ann', '1153.txt', '1154.ann', '1154.txt', '1155.ann', '1155.txt', '1156.ann', '1156.txt', '1157.ann', '1157.txt', '1158.ann', '1158.txt', '1159.ann', '1159.txt', '116.ann', '116.txt', '1160.ann', '1160.txt', '1161.ann', '1161.txt', '1162.ann', '1162.txt', '1163.ann', '1163.txt', '1164.ann', '1164.txt', '1165.ann', '1165.txt', '1166.ann', '1166.txt', '1167.ann', '1167.txt', '1168.ann', '1168.txt', '1169.ann', '1169.txt', '117.ann', '117.txt', '1170.ann', '1170.txt', '1171.ann', '1171.txt', '1172.ann', '1172.txt', '1173.ann', '1173.txt', '1174.ann', '1174.txt', '1175.ann', '1175.txt', '1176.ann', '1176.txt', '1177.ann', '1177.txt', '1178.ann', '1178.txt', '1179.ann', '1179.txt', '118.ann', '118.txt', '1180.ann', '1180.txt', '1181.ann', '1181.txt', '1182.ann', '1182.txt', '1183.ann', '1183.txt', '1184.ann', '1184.txt', '1185.ann', '1185.txt', '1186.ann', '1186.txt', '1187.ann', '1187.txt', '1188.ann', '1188.txt', '1189.ann', '1189.txt', '119.ann', '119.txt', '1190.ann', '1190.txt', '1191.ann', '1191.txt', '1192.ann', '1192.txt', '1193.ann', '1193.txt', '1194.ann', '1194.txt', '1195.ann', '1195.txt', '1196.ann', '1196.txt', '1197.ann', '1197.txt', '1198.ann', '1198.txt', '1199.ann', '1199.txt', '11_01_13b.ann', '11_01_13b.txt', '11_01_13e.ann', '11_01_13e.txt', '120.ann', '120.txt', '1200.ann', '1200.txt', '121.ann', '121.txt', '122.ann', '122.txt', '123.ann', '123.txt', '124.ann', '124.txt', '125.ann', '125.txt', '126.ann', '126.txt', '127.ann', '127.txt', '128.ann', '128.txt', '129.ann', '129.txt', '130.ann', '130.txt', '131.ann', '131.txt', '132.ann', '132.txt', '133.ann', '133.txt', '134.ann', '134.txt', '135.ann', '135.txt', '136.ann', '136.txt', '137.ann', '137.txt', '138.ann', '138.txt', '139.ann', '139.txt', '140.ann', '140.txt', '141.ann', '141.txt', '142.ann', '142.txt', '143.ann', '143.txt', '144.ann', '144.txt', '145.ann', '145.txt', '146.ann', '146.txt', '147.ann', '147.txt', '148.ann', '148.txt', '149.ann', '149.txt', '14_01_13c.ann', '14_01_13c.txt', '14_01_13g.ann', '14_01_13g.txt', '14_01_13i.ann', '14_01_13i.txt', '150.ann', '150.txt', '151.ann', '151.txt', '152.ann', '152.txt', '153.ann', '153.txt', '154.ann', '154.txt', '155.ann', '155.txt', '156.ann', '156.txt', '157.ann', '157.txt', '158.ann', '158.txt', '159.ann', '159.txt', '15_01_13a.ann', '15_01_13a.txt', '15_01_13b.ann', '15_01_13b.txt', '15_01_13e.ann', '15_01_13e.txt', '15_01_13f.ann', '15_01_13f.txt', '160.ann', '160.txt', '161.ann', '161.txt', '162.ann', '162.txt', '163.ann', '163.txt', '164.ann', '164.txt', '165.ann', '165.txt', '166.ann', '166.txt', '167.ann', '167.txt', '168.ann', '168.txt', '169.ann', '169.txt', '170.ann', '170.txt', '171.ann', '171.txt', '172.ann', '172.txt', '173.ann', '173.txt', '174.ann', '174.txt', '175.ann', '175.txt', '176.ann', '176.txt', '177.ann', '177.txt', '178.ann', '178.txt', '179.ann', '179.txt', '180.ann', '180.txt', '181.ann', '181.txt', '182.ann', '182.txt', '183.ann', '183.txt', '184.ann', '184.txt', '185.ann', '185.txt', '186.ann', '186.txt', '187.ann', '187.txt', '188.ann', '188.txt', '189.ann', '189.txt', '190.ann', '190.txt', '191.ann', '191.txt', '192.ann', '192.txt', '193.ann', '193.txt', '194.ann', '194.txt', '195.ann', '195.txt', '196.ann', '196.txt', '197.ann', '197.txt', '198.ann', '198.txt', '199.ann', '199.txt', '19_11_12d.ann', '19_11_12d.txt', '19_11_12h.ann', '19_11_12h.txt', '200.ann', '200.txt', '2001.ann', '2001.txt', '2002.ann', '2002.txt', '2003.ann', '2003.txt', '2004.ann', '2004.txt', '2005.ann', '2005.txt', '2006.ann', '2006.txt', '2007.ann', '2007.txt', '2008.ann', '2008.txt', '2009.ann', '2009.txt', '201.ann', '201.txt', '2010.ann', '2010.txt', '2011.ann', '2011.txt', '2012.ann', '2012.txt', '2013.ann', '2013.txt', '2014.ann', '2014.txt', '2015.ann', '2015.txt', '2016.ann', '2016.txt', '2017.ann', '2017.txt', '2018.ann', '2018.txt', '2019.ann', '2019.txt', '202.ann', '202.txt', '2020.ann', '2020.txt', '2021.ann', '2021.txt', '2022.ann', '2022.txt', '2023.ann', '2023.txt', '2024.ann', '2024.txt', '2025.ann', '2025.txt', '2026.ann', '2026.txt', '2027.ann', '2027.txt', '2028.ann', '2028.txt', '2029.ann', '2029.txt', '203.ann', '203.txt', '2030.ann', '2030.txt', '2031.ann', '2031.txt', '2032.ann', '2032.txt', '2034.ann', '2034.txt', '2035.ann', '2035.txt', '2036.ann', '2036.txt', '2037.ann', '2037.txt', '2038.ann', '2038.txt', '2039.ann', '2039.txt', '204.ann', '204.txt', '2040.ann', '2040.txt', '2041.ann', '2041.txt', '2042.ann', '2042.txt', '2043.ann', '2043.txt', '2044.ann', '2044.txt', '2045.ann', '2045.txt', '2046.ann', '2046.txt', '2047.ann', '2047.txt', '2048.ann', '2048.txt', '2049.ann', '2049.txt', '205.ann', '205.txt', '2050.ann', '2050.txt', '206.ann', '206.txt', '207.ann', '207.txt', '208.ann', '208.txt', '209.ann', '209.txt', '20_11_12a.ann', '20_11_12a.txt', '20_11_12b.ann', '20_11_12b.txt', '20_11_12c.ann', '20_11_12c.txt', '20_11_12d.ann', '20_11_12d.txt', '20_11_12i.ann', '20_11_12i.txt', '210.ann', '210.txt', '211.ann', '211.txt', '212.ann', '212.txt', '213.ann', '213.txt', '214.ann', '214.txt', '215.ann', '215.txt', '216.ann', '216.txt', '217.ann', '217.txt', '218.ann', '218.txt', '219.ann', '219.txt', '21_11_12c.ann', '21_11_12c.txt', '21_11_12h.ann', '21_11_12h.txt', '21_11_12i.ann', '21_11_12i.txt', '21_11_12j.ann', '21_11_12j.txt', '220.ann', '220.txt', '221.ann', '221.txt', '222.ann', '222.txt', '223.ann', '223.txt', '224.ann', '224.txt', '225.ann', '225.txt', '226.ann', '226.txt', '227.ann', '227.txt', '228.ann', '228.txt', '229.ann', '229.txt', '22_11_12a.ann', '22_11_12a.txt', '22_11_12c.ann', '22_11_12c.txt', '22_11_12d.ann', '22_11_12d.txt', '22_11_12g.ann', '22_11_12g.txt', '22_11_12h.ann', '22_11_12h.txt', '22_11_12i.ann', '22_11_12i.txt', '22_11_12j.ann', '22_11_12j.txt', '230.ann', '230.txt', '231.ann', '231.txt', '232.ann', '232.txt', '233.ann', '233.txt', '234.ann', '234.txt', '235.ann', '235.txt', '236.ann', '236.txt', '237.ann', '237.txt', '238.ann', '238.txt', '239.ann', '239.txt', '23_11_12a.ann', '23_11_12a.txt', '23_11_12b.ann', '23_11_12b.txt', '23_11_12c.ann', '23_11_12c.txt', '23_11_12d.ann', '23_11_12d.txt', '23_11_12e.ann', '23_11_12e.txt', '23_11_12f.ann', '23_11_12f.txt', '240.ann', '240.txt', '241.ann', '241.txt', '242.ann', '242.txt', '243.ann', '243.txt', '244.ann', '244.txt', '245.ann', '245.txt', '246.ann', '246.txt', '247.ann', '247.txt', '248.ann', '248.txt', '249.ann', '249.txt', '250.ann', '250.txt', '251.ann', '251.txt', '252.ann', '252.txt', '253.ann', '253.txt', '254.ann', '254.txt', '255.ann', '255.txt', '256.ann', '256.txt', '257.ann', '257.txt', '258.ann', '258.txt', '259.ann', '259.txt', '25_12_12a.ann', '25_12_12a.txt', '25_12_12c.ann', '25_12_12c.txt', '25_12_12d.ann', '25_12_12d.txt', '25_12_12e.ann', '25_12_12e.txt', '260.ann', '260.txt', '261.ann', '261.txt', '262.ann', '262.txt', '263.ann', '263.txt', '264.ann', '264.txt', '265.ann', '265.txt', '266.ann', '266.txt', '267.ann', '267.txt', '268.ann', '268.txt', '269.ann', '269.txt', '26_11_12b.ann', '26_11_12b.txt', '26_11_12c.ann', '26_11_12c.txt', '26_11_12e.ann', '26_11_12e.txt', '26_11_12f.ann', '26_11_12f.txt', '270.ann', '270.txt', '271.ann', '271.txt', '272.ann', '272.txt', '273.ann', '273.txt', '274.ann', '274.txt', '275.ann', '275.txt', '276.ann', '276.txt', '277.ann', '277.txt', '278.ann', '278.txt', '279.ann', '279.txt', '27_11_12a.ann', '27_11_12a.txt', '27_11_12c.ann', '27_11_12c.txt', '27_11_12d.ann', '27_11_12d.txt', '27_11_12e.ann', '27_11_12e.txt', '27_11_12j.ann', '27_11_12j.txt', '280.ann', '280.txt', '281.ann', '281.txt', '282.ann', '282.txt', '283.ann', '283.txt', '284.ann', '284.txt', '285.ann', '285.txt', '286.ann', '286.txt', '287.ann', '287.txt', '288.ann', '288.txt', '289.ann', '289.txt', '28_11_12a.ann', '28_11_12a.txt', '28_11_12f.ann', '28_11_12f.txt', '28_11_12g.ann', '28_11_12g.txt', '28_11_12h.ann', '28_11_12h.txt', '28_11_12i.ann', '28_11_12i.txt', '28_11_12j.ann', '28_11_12j.txt', '290.ann', '290.txt', '291.ann', '291.txt', '292.ann', '292.txt', '293.ann', '293.txt', '294.ann', '294.txt', '295.ann', '295.txt', '296.ann', '296.txt', '297.ann', '297.txt', '298.ann', '298.txt', '299.ann', '299.txt', '29_11_12a.ann', '29_11_12a.txt', '29_11_12b.ann', '29_11_12b.txt', '300.ann', '300.txt', '301.ann', '301.txt', '302.ann', '302.txt', '303.ann', '303.txt', '304.ann', '304.txt', '305.ann', '305.txt', '306.ann', '306.txt', '307.ann', '307.txt', '308.ann', '308.txt', '309.ann', '309.txt', '30_11_12b.ann', '30_11_12b.txt', '30_11_12h.ann', '30_11_12h.txt', '30_11_12i.ann', '30_11_12i.txt', '310.ann', '310.txt', '311.ann', '311.txt', '312.ann', '312.txt', '313.ann', '313.txt', '314.ann', '314.txt', '315.ann', '315.txt', '316.ann', '316.txt', '317.ann', '317.txt', '318.ann', '318.txt', '319.ann', '319.txt', '320.ann', '320.txt', '321.ann', '321.txt', '322.ann', '322.txt', '323.ann', '323.txt', '324.ann', '324.txt', '325.ann', '325.txt', '326.ann', '326.txt', '327.ann', '327.txt', '328.ann', '328.txt', '329.ann', '329.txt', '330.ann', '330.txt', '331.ann', '331.txt', '332.ann', '332.txt', '333.ann', '333.txt', '334.ann', '334.txt', '335.ann', '335.txt', '336.ann', '336.txt', '337.ann', '337.txt', '338.ann', '338.txt', '339.ann', '339.txt', '340.ann', '340.txt', '341.ann', '341.txt', '342.ann', '342.txt', '343.ann', '343.txt', '344.ann', '344.txt', '345.ann', '345.txt', '346.ann', '346.txt', '347.ann', '347.txt', '348.ann', '348.txt', '349.ann', '349.txt', '350.ann', '350.txt', '351.ann', '351.txt', '352.ann', '352.txt', '353.ann', '353.txt', '354.ann', '354.txt', '355.ann', '355.txt', '356.ann', '356.txt', '357.ann', '357.txt', '358.ann', '358.txt', '359.ann', '359.txt', '360.ann', '360.txt', '361.ann', '361.txt', '362.ann', '362.txt', '363.ann', '363.txt', '364.ann', '364.txt', '365.ann', '365.txt', '366.ann', '366.txt', '367.ann', '367.txt', '368.ann', '368.txt', '369.ann', '369.txt', '370.ann', '370.txt', '371.ann', '371.txt', '372.ann', '372.txt', '373.ann', '373.txt', '374.ann', '374.txt', '375.ann', '375.txt', '376.ann', '376.txt', '377.ann', '377.txt', '378.ann', '378.txt', '379.ann', '379.txt', '380.ann', '380.txt', '381.ann', '381.txt', '382.ann', '382.txt', '383.ann', '383.txt', '384.ann', '384.txt', '385.ann', '385.txt', '386.ann', '386.txt', '387.ann', '387.txt', '388.ann', '388.txt', '389.ann', '389.txt', '390.ann', '390.txt', '391.ann', '391.txt', '392.ann', '392.txt', '393.ann', '393.txt', '394.ann', '394.txt', '395.ann', '395.txt', '396.ann', '396.txt', '397.ann', '397.txt', '398.ann', '398.txt', '399.ann', '399.txt', '400.ann', '400.txt', '401.ann', '401.txt', '402.ann', '402.txt', '403.ann', '403.txt', '404.ann', '404.txt', '405.ann', '405.txt', '406.ann', '406.txt', '407.ann', '407.txt', '408.ann', '408.txt', '409.ann', '409.txt', '410.ann', '410.txt', '411.ann', '411.txt', '412.ann', '412.txt', '413.ann', '413.txt', '414.ann', '414.txt', '415.ann', '415.txt', '416.ann', '416.txt', '417.ann', '417.txt', '418.ann', '418.txt', '419.ann', '419.txt', '420.ann', '420.txt', '421.ann', '421.txt', '422.ann', '422.txt', '423.ann', '423.txt', '424.ann', '424.txt', '425.ann', '425.txt', '426.ann', '426.txt', '427.ann', '427.txt', '428.ann', '428.txt', '429.ann', '429.txt', '430.ann', '430.txt', '431.ann', '431.txt', '432.ann', '432.txt', '433.ann', '433.txt', '434.ann', '434.txt', '435.ann', '435.txt', '436.ann', '436.txt', '437.ann', '437.txt', '438.ann', '438.txt', '439.ann', '439.txt', '440.ann', '440.txt', '441.ann', '441.txt', '442.ann', '442.txt', '443.ann', '443.txt', '444.ann', '444.txt', '445.ann', '445.txt', '446.ann', '446.txt', '447.ann', '447.txt', '448.ann', '448.txt', '449.ann', '449.txt', '450.ann', '450.txt', '451.ann', '451.txt', '452.ann', '452.txt', '453.ann', '453.txt', '454.ann', '454.txt', '455.ann', '455.txt', '457.ann', '457.txt', '458.ann', '458.txt', '459.ann', '459.txt', '460.ann', '460.txt', '461.ann', '461.txt', '462.ann', '462.txt', '463.ann', '463.txt', '464.ann', '464.txt', '465.ann', '465.txt', '466.ann', '466.txt', '467.ann', '467.txt', '468.ann', '468.txt', '469.ann', '469.txt', '470.ann', '470.txt', '471.ann', '471.txt', '472.ann', '472.txt', '473.ann', '473.txt', '474.ann', '474.txt', '475.ann', '475.txt', '476.ann', '476.txt', '477.ann', '477.txt', '478.ann', '478.txt', '479.ann', '479.txt', '480.ann', '480.txt', '481.ann', '481.txt', '482.ann', '482.txt', '483.ann', '483.txt', '484.ann', '484.txt', '485.ann', '485.txt', '486.ann', '486.txt', '487.ann', '487.txt', '488.ann', '488.txt', '489.ann', '489.txt', '490.ann', '490.txt', '491.ann', '491.txt', '492.ann', '492.txt', '493.ann', '493.txt', '494.ann', '494.txt', '495.ann', '495.txt', '496.ann', '496.txt', '497.ann', '497.txt', '498.ann', '498.txt', '499.ann', '499.txt', '500.ann', '500.txt', '501.ann', '501.txt', '502.ann', '502.txt', '503.ann', '503.txt', '504.ann', '504.txt', '505.ann', '505.txt', '506.ann', '506.txt', '507.ann', '507.txt', '508.ann', '508.txt', '509.ann', '509.txt', '510.ann', '510.txt', '511.ann', '511.txt', '512.ann', '512.txt', '513.ann', '513.txt', '514.ann', '514.txt', '515.ann', '515.txt', '516.ann', '516.txt', '517.ann', '517.txt', '518.ann', '518.txt', '519.ann', '519.txt', '520.ann', '520.txt', '521.ann', '521.txt', '522.ann', '522.txt', '523.ann', '523.txt', '524.ann', '524.txt', '525.ann', '525.txt', '526.ann', '526.txt', '527.ann', '527.txt', '528.ann', '528.txt', '529.ann', '529.txt', '530.ann', '530.txt', '531.ann', '531.txt', '532.ann', '532.txt', '533 (!).ann', '533 (!).txt', '534.ann', '534.txt', '535.ann', '535.txt', '536.ann', '536.txt', '537.ann', '537.txt', '538.ann', '538.txt', '539.ann', '539.txt', '540.ann', '540.txt', '541.ann', '541.txt', '542.ann', '542.txt', '543.ann', '543.txt', '544.ann', '544.txt', '545.ann', '545.txt', '546.ann', '546.txt', '547.ann', '547.txt', '548.ann', '548.txt', '549.ann', '549.txt', '550.ann', '550.txt', '551.ann', '551.txt', '552.ann', '552.txt', '553.ann', '553.txt', '554.ann', '554.txt', '555 (!).ann', '555 (!).txt', '556.ann', '556.txt', '557.ann', '557.txt', '558.ann', '558.txt', '559.ann', '559.txt', '560.ann', '560.txt', '561.ann', '561.txt', '562.ann', '562.txt', '563.ann', '563.txt', '564.ann', '564.txt', '565.ann', '565.txt', '567.ann', '567.txt', '568.ann', '568.txt', '569.ann', '569.txt', '570.ann', '570.txt', '571.ann', '571.txt', '572.ann', '572.txt', '574.ann', '574.txt', '575.ann', '575.txt', '576.ann', '576.txt', '577.ann', '577.txt', '578.ann', '578.txt', '579.ann', '579.txt', '581.ann', '581.txt', '582.ann', '582.txt', '583.ann', '583.txt', '584 (!).ann', '584 (!).txt', '585.ann', '585.txt', '586.ann', '586.txt', '587.ann', '587.txt', '588.ann', '588.txt', '589.ann', '589.txt', '590.ann', '590.txt', '591.ann', '591.txt', '592.ann', '592.txt', '593.ann', '593.txt', '594.ann', '594.txt', '596.ann', '596.txt', '597.ann', '597.txt', '598 (!).ann', '598 (!).txt', '599.ann', '599.txt', '600.ann', '600.txt', '601.ann', '601.txt', '602.ann', '602.txt', '610.ann', '610.txt', '611.ann', '611.txt', '612.ann', '612.txt', '613.ann', '613.txt', '614.ann', '614.txt', '615.ann', '615.txt', '616.ann', '616.txt', '617.ann', '617.txt', '618.ann', '618.txt', '619.ann', '619.txt', '620.ann', '620.txt', '621.ann', '621.txt', '622.ann', '622.txt', '623.ann', '623.txt', '624.ann', '624.txt', '625.ann', '625.txt', '626.ann', '626.txt', '627.ann', '627.txt', '628.ann', '628.txt', '629.ann', '629.txt', '630.ann', '630.txt', '631.ann', '631.txt', '632.ann', '632.txt', '633.ann', '633.txt', 'abdulatipov.ann', 'abdulatipov.txt', 'artjakov.ann', 'artjakov.txt', 'Avtovaz.ann', 'Avtovaz.txt', 'blokhin.ann', 'blokhin.txt', 'chaves.ann', 'chaves.txt', 'chirkunov.ann', 'chirkunov.txt', 'kamchatka.ann', 'kamchatka.txt', 'klinton.ann', 'klinton.txt', 'kuleshov.ann', 'kuleshov.txt', 'last_01.ann', 'last_01.txt', 'last_02.ann', 'last_02.txt', 'last_03.ann', 'last_03.txt', 'last_04.ann', 'last_04.txt', 'last_05.ann', 'last_05.txt', 'last_06.ann', 'last_06.txt', 'last_07_new.ann', 'last_07_new.txt', 'last_08.ann', 'last_08.txt', 'last_09.ann', 'last_09.txt', 'last_10.ann', 'last_10.txt', 'last_11.ann', 'last_11.txt', 'last_12.ann', 'last_12.txt', 'last_13.ann', 'last_13.txt', 'last_14.ann', 'last_14.txt', 'last_15.ann', 'last_15.txt', 'last_16.ann', 'last_16.txt', 'last_17.ann', 'last_17.txt', 'last_18.ann', 'last_18.txt', 'last_19.ann', 'last_19.txt', 'last_20.ann', 'last_20.txt', 'last_21.ann', 'last_21.txt', 'last_22.ann', 'last_22.txt', 'last_23.ann', 'last_23.txt', 'last_24.ann', 'last_24.txt', 'last_25.ann', 'last_25.txt', 'last_26.ann', 'last_26.txt', 'last_27.ann', 'last_27.txt', 'last_28.ann', 'last_28.txt', 'last_29.ann', 'last_29.txt', 'last_30_new.ann', 'last_30_new.txt', 'last_31.ann', 'last_31.txt', 'last_32.ann', 'last_32.txt', 'last_33.ann', 'last_33.txt', 'last_34.ann', 'last_34.txt', 'last_35.ann', 'last_35.txt', 'last_36.ann', 'last_36.txt', 'last_37.ann', 'last_37.txt', 'last_38.ann', 'last_38.txt', 'last_39.ann', 'last_39.txt', 'last_40.ann', 'last_40.txt', 'last_41.ann', 'last_41.txt', 'last_42.ann', 'last_42.txt', 'last_43.ann', 'last_43.txt', 'last_44.ann', 'last_44.txt', 'last_45.ann', 'last_45.txt', 'last_46.ann', 'last_46.txt', 'last_47.ann', 'last_47.txt', 'last_48.ann', 'last_48.txt', 'last_49.ann', 'last_49.txt', 'last_50.ann', 'last_50.txt', 'last_51.ann', 'last_51.txt', 'last_52.ann', 'last_52.txt', 'last_53.ann', 'last_53.txt', 'last_54.ann', 'last_54.txt', 'last_55.ann', 'last_55.txt', 'last_56.ann', 'last_56.txt', 'last_57.ann', 'last_57.txt', 'last_58.ann', 'last_58.txt', 'last_59.ann', 'last_59.txt', 'last_60.ann', 'last_60.txt', 'last_61.ann', 'last_61.txt', 'last_62.ann', 'last_62.txt', 'last_63.ann', 'last_63.txt', 'last_64.ann', 'last_64.txt', 'last_65.ann', 'last_65.txt', 'last_66.ann', 'last_66.txt', 'last_67.ann', 'last_67.txt', 'last_68.ann', 'last_68.txt', 'last_69.ann', 'last_69.txt', 'last_70.ann', 'last_70.txt', 'last_71.ann', 'last_71.txt', 'last_72.ann', 'last_72.txt', 'last_73.ann', 'last_73.txt', 'last_74.ann', 'last_74.txt', 'last_75.ann', 'last_75.txt', 'lenoblast.ann', 'lenoblast.txt', 'maykl dzhekson.ann', 'maykl dzhekson.txt', 'mvd.ann', 'mvd.txt', 'mvd2.ann', 'mvd2.txt', 'rosobrnadzor.ann', 'rosobrnadzor.txt', 'ryadovoy chelah.ann', 'ryadovoy chelah.txt', 'semenenko.ann', 'semenenko.txt', 'shojgu1.ann', 'shojgu1.txt', 'shojgu3.ann', 'shojgu3.txt', 'shojgu4.ann', 'shojgu4.txt', 'shojgu6.ann', 'shojgu6.txt', 'si_tzjanpin.ann', 'si_tzjanpin.txt', 'sobjanin2.ann', 'sobjanin2.txt', 'turkmenija.ann', 'turkmenija.txt', 'uchitel.ann', 'uchitel.txt']\n"
     ]
    }
   ],
   "source": [
    "# Просматриваем содержание коллекции\n",
    "print(os.listdir(\"Collection5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e48f3aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.txt', '002.txt', '003.txt', '004.txt', '005.txt', '006.txt', '007.txt', '008.txt', '009.txt', '010.txt', '011.txt', '012.txt', '013.txt', '014.txt', '015 (!).txt', '016.txt', '017.txt', '018.txt', '019.txt', '020.txt', '021.txt', '022.txt', '023.txt', '025.txt', '026.txt', '027.txt', '028.txt', '029.txt', '030.txt', '031.txt', '032.txt', '033.txt', '034.txt', '035.txt', '036.txt', '037.txt', '038.txt', '039.txt', '03_12_12a.txt', '03_12_12b.txt', '03_12_12c.txt', '03_12_12d.txt', '03_12_12g.txt', '03_12_12h.txt', '040.txt', '041.txt', '042.txt', '043.txt', '044.txt', '045.txt', '046.txt', '047.txt', '048.txt', '049.txt', '04_02_13a_abdulatipov.txt', '04_03_13a_sorokin.txt', '04_12_12b.txt', '04_12_12d.txt', '04_12_12f.txt', '04_12_12g.txt', '04_12_12h_corr.txt', '050.txt', '051.txt', '052.txt', '053.txt', '054.txt', '055.txt', '056.txt', '057.txt', '058.txt', '059.txt', '060.txt', '061.txt', '062.txt', '063.txt', '064.txt', '065.txt', '066.txt', '067.txt', '068.txt', '069.txt', '070.txt', '071.txt', '072.txt', '073.txt', '074.txt', '075.txt', '076.txt', '077.txt', '078.txt', '079.txt', '080.txt', '081.txt', '082.txt', '083.txt', '084.txt', '085.txt', '086.txt', '087.txt', '088.txt', '089.txt', '090.txt', '091.txt', '092.txt', '093.txt', '094.txt', '095.txt', '096.txt', '097.txt', '098.txt', '099.txt', '09_01_13.txt', '09_01_13a.txt', '09_01_13c.txt', '09_01_13d.txt', '09_01_13e.txt', '09_01_13h.txt', '09_01_13i.txt', '100.txt', '1000.txt', '1001.txt', '1002.txt', '1003.txt', '1004.txt', '1005.txt', '1006.txt', '1007.txt', '1008.txt', '1009.txt', '101.txt', '1010.txt', '1011.txt', '1012.txt', '1013.txt', '1014.txt', '1015.txt', '1016.txt', '1017.txt', '1018.txt', '1019.txt', '102.txt', '1020.txt', '1021.txt', '1022.txt', '1023.txt', '1024.txt', '1025.txt', '1026.txt', '1027.txt', '1028.txt', '1029.txt', '103.txt', '1030.txt', '1031.txt', '1032.txt', '1033.txt', '1034.txt', '1035.txt', '1036.txt', '1037.txt', '1038.txt', '1039.txt', '104.txt', '1040.txt', '1041.txt', '1042.txt', '1043.txt', '1044.txt', '1045.txt', '1046.txt', '1047.txt', '1048.txt', '1049.txt', '105.txt', '1050.txt', '106.txt', '107.txt', '108.txt', '109.txt', '10_01_13a.txt', '10_01_13d.txt', '10_01_13i.txt', '110.txt', '1100.txt', '1101.txt', '1102.txt', '1103.txt', '1104.txt', '1105.txt', '1106.txt', '1107.txt', '1108.txt', '1109.txt', '111.txt', '1110.txt', '1111.txt', '1112.txt', '1113.txt', '1114.txt', '1115.txt', '1116.txt', '1117.txt', '1118.txt', '1119.txt', '112.txt', '1120.txt', '1121.txt', '1122.txt', '1123.txt', '1124.txt', '1125.txt', '1126.txt', '1127.txt', '1128.txt', '113.txt', '1130.txt', '1131.txt', '1132.txt', '1133.txt', '1134.txt', '1135.txt', '1136.txt', '1137.txt', '1138.txt', '1139.txt', '114.txt', '1140.txt', '1141.txt', '1142.txt', '1143.txt', '1144.txt', '1145.txt', '1146.txt', '1147.txt', '1148.txt', '1149.txt', '115.txt', '1150.txt', '1151.txt', '1152.txt', '1153.txt', '1154.txt', '1155.txt', '1156.txt', '1157.txt', '1158.txt', '1159.txt', '116.txt', '1160.txt', '1161.txt', '1162.txt', '1163.txt', '1164.txt', '1165.txt', '1166.txt', '1167.txt', '1168.txt', '1169.txt', '117.txt', '1170.txt', '1171.txt', '1172.txt', '1173.txt', '1174.txt', '1175.txt', '1176.txt', '1177.txt', '1178.txt', '1179.txt', '118.txt', '1180.txt', '1181.txt', '1182.txt', '1183.txt', '1184.txt', '1185.txt', '1186.txt', '1187.txt', '1188.txt', '1189.txt', '119.txt', '1190.txt', '1191.txt', '1192.txt', '1193.txt', '1194.txt', '1195.txt', '1196.txt', '1197.txt', '1198.txt', '1199.txt', '11_01_13b.txt', '11_01_13e.txt', '120.txt', '1200.txt', '121.txt', '122.txt', '123.txt', '124.txt', '125.txt', '126.txt', '127.txt', '128.txt', '129.txt', '130.txt', '131.txt', '132.txt', '133.txt', '134.txt', '135.txt', '136.txt', '137.txt', '138.txt', '139.txt', '140.txt', '141.txt', '142.txt', '143.txt', '144.txt', '145.txt', '146.txt', '147.txt', '148.txt', '149.txt', '14_01_13c.txt', '14_01_13g.txt', '14_01_13i.txt', '150.txt', '151.txt', '152.txt', '153.txt', '154.txt', '155.txt', '156.txt', '157.txt', '158.txt', '159.txt', '15_01_13a.txt', '15_01_13b.txt', '15_01_13e.txt', '15_01_13f.txt', '160.txt', '161.txt', '162.txt', '163.txt', '164.txt', '165.txt', '166.txt', '167.txt', '168.txt', '169.txt', '170.txt', '171.txt', '172.txt', '173.txt', '174.txt', '175.txt', '176.txt', '177.txt', '178.txt', '179.txt', '180.txt', '181.txt', '182.txt', '183.txt', '184.txt', '185.txt', '186.txt', '187.txt', '188.txt', '189.txt', '190.txt', '191.txt', '192.txt', '193.txt', '194.txt', '195.txt', '196.txt', '197.txt', '198.txt', '199.txt', '19_11_12d.txt', '19_11_12h.txt', '200.txt', '2001.txt', '2002.txt', '2003.txt', '2004.txt', '2005.txt', '2006.txt', '2007.txt', '2008.txt', '2009.txt', '201.txt', '2010.txt', '2011.txt', '2012.txt', '2013.txt', '2014.txt', '2015.txt', '2016.txt', '2017.txt', '2018.txt', '2019.txt', '202.txt', '2020.txt', '2021.txt', '2022.txt', '2023.txt', '2024.txt', '2025.txt', '2026.txt', '2027.txt', '2028.txt', '2029.txt', '203.txt', '2030.txt', '2031.txt', '2032.txt', '2034.txt', '2035.txt', '2036.txt', '2037.txt', '2038.txt', '2039.txt', '204.txt', '2040.txt', '2041.txt', '2042.txt', '2043.txt', '2044.txt', '2045.txt', '2046.txt', '2047.txt', '2048.txt', '2049.txt', '205.txt', '2050.txt', '206.txt', '207.txt', '208.txt', '209.txt', '20_11_12a.txt', '20_11_12b.txt', '20_11_12c.txt', '20_11_12d.txt', '20_11_12i.txt', '210.txt', '211.txt', '212.txt', '213.txt', '214.txt', '215.txt', '216.txt', '217.txt', '218.txt', '219.txt', '21_11_12c.txt', '21_11_12h.txt', '21_11_12i.txt', '21_11_12j.txt', '220.txt', '221.txt', '222.txt', '223.txt', '224.txt', '225.txt', '226.txt', '227.txt', '228.txt', '229.txt', '22_11_12a.txt', '22_11_12c.txt', '22_11_12d.txt', '22_11_12g.txt', '22_11_12h.txt', '22_11_12i.txt', '22_11_12j.txt', '230.txt', '231.txt', '232.txt', '233.txt', '234.txt', '235.txt', '236.txt', '237.txt', '238.txt', '239.txt', '23_11_12a.txt', '23_11_12b.txt', '23_11_12c.txt', '23_11_12d.txt', '23_11_12e.txt', '23_11_12f.txt', '240.txt', '241.txt', '242.txt', '243.txt', '244.txt', '245.txt', '246.txt', '247.txt', '248.txt', '249.txt', '250.txt', '251.txt', '252.txt', '253.txt', '254.txt', '255.txt', '256.txt', '257.txt', '258.txt', '259.txt', '25_12_12a.txt', '25_12_12c.txt', '25_12_12d.txt', '25_12_12e.txt', '260.txt', '261.txt', '262.txt', '263.txt', '264.txt', '265.txt', '266.txt', '267.txt', '268.txt', '269.txt', '26_11_12b.txt', '26_11_12c.txt', '26_11_12e.txt', '26_11_12f.txt', '270.txt', '271.txt', '272.txt', '273.txt', '274.txt', '275.txt', '276.txt', '277.txt', '278.txt', '279.txt', '27_11_12a.txt', '27_11_12c.txt', '27_11_12d.txt', '27_11_12e.txt', '27_11_12j.txt', '280.txt', '281.txt', '282.txt', '283.txt', '284.txt', '285.txt', '286.txt', '287.txt', '288.txt', '289.txt', '28_11_12a.txt', '28_11_12f.txt', '28_11_12g.txt', '28_11_12h.txt', '28_11_12i.txt', '28_11_12j.txt', '290.txt', '291.txt', '292.txt', '293.txt', '294.txt', '295.txt', '296.txt', '297.txt', '298.txt', '299.txt', '29_11_12a.txt', '29_11_12b.txt', '300.txt', '301.txt', '302.txt', '303.txt', '304.txt', '305.txt', '306.txt', '307.txt', '308.txt', '309.txt', '30_11_12b.txt', '30_11_12h.txt', '30_11_12i.txt', '310.txt', '311.txt', '312.txt', '313.txt', '314.txt', '315.txt', '316.txt', '317.txt', '318.txt', '319.txt', '320.txt', '321.txt', '322.txt', '323.txt', '324.txt', '325.txt', '326.txt', '327.txt', '328.txt', '329.txt', '330.txt', '331.txt', '332.txt', '333.txt', '334.txt', '335.txt', '336.txt', '337.txt', '338.txt', '339.txt', '340.txt', '341.txt', '342.txt', '343.txt', '344.txt', '345.txt', '346.txt', '347.txt', '348.txt', '349.txt', '350.txt', '351.txt', '352.txt', '353.txt', '354.txt', '355.txt', '356.txt', '357.txt', '358.txt', '359.txt', '360.txt', '361.txt', '362.txt', '363.txt', '364.txt', '365.txt', '366.txt', '367.txt', '368.txt', '369.txt', '370.txt', '371.txt', '372.txt', '373.txt', '374.txt', '375.txt', '376.txt', '377.txt', '378.txt', '379.txt', '380.txt', '381.txt', '382.txt', '383.txt', '384.txt', '385.txt', '386.txt', '387.txt', '388.txt', '389.txt', '390.txt', '391.txt', '392.txt', '393.txt', '394.txt', '395.txt', '396.txt', '397.txt', '398.txt', '399.txt', '400.txt', '401.txt', '402.txt', '403.txt', '404.txt', '405.txt', '406.txt', '407.txt', '408.txt', '409.txt', '410.txt', '411.txt', '412.txt', '413.txt', '414.txt', '415.txt', '416.txt', '417.txt', '418.txt', '419.txt', '420.txt', '421.txt', '422.txt', '423.txt', '424.txt', '425.txt', '426.txt', '427.txt', '428.txt', '429.txt', '430.txt', '431.txt', '432.txt', '433.txt', '434.txt', '435.txt', '436.txt', '437.txt', '438.txt', '439.txt', '440.txt', '441.txt', '442.txt', '443.txt', '444.txt', '445.txt', '446.txt', '447.txt', '448.txt', '449.txt', '450.txt', '451.txt', '452.txt', '453.txt', '454.txt', '455.txt', '457.txt', '458.txt', '459.txt', '460.txt', '461.txt', '462.txt', '463.txt', '464.txt', '465.txt', '466.txt', '467.txt', '468.txt', '469.txt', '470.txt', '471.txt', '472.txt', '473.txt', '474.txt', '475.txt', '476.txt', '477.txt', '478.txt', '479.txt', '480.txt', '481.txt', '482.txt', '483.txt', '484.txt', '485.txt', '486.txt', '487.txt', '488.txt', '489.txt', '490.txt', '491.txt', '492.txt', '493.txt', '494.txt', '495.txt', '496.txt', '497.txt', '498.txt', '499.txt', '500.txt', '501.txt', '502.txt', '503.txt', '504.txt', '505.txt', '506.txt', '507.txt', '508.txt', '509.txt', '510.txt', '511.txt', '512.txt', '513.txt', '514.txt', '515.txt', '516.txt', '517.txt', '518.txt', '519.txt', '520.txt', '521.txt', '522.txt', '523.txt', '524.txt', '525.txt', '526.txt', '527.txt', '528.txt', '529.txt', '530.txt', '531.txt', '532.txt', '533 (!).txt', '534.txt', '535.txt', '536.txt', '537.txt', '538.txt', '539.txt', '540.txt', '541.txt', '542.txt', '543.txt', '544.txt', '545.txt', '546.txt', '547.txt', '548.txt', '549.txt', '550.txt', '551.txt', '552.txt', '553.txt', '554.txt', '555 (!).txt', '556.txt', '557.txt', '558.txt', '559.txt', '560.txt', '561.txt', '562.txt', '563.txt', '564.txt', '565.txt', '567.txt', '568.txt', '569.txt', '570.txt', '571.txt', '572.txt', '574.txt', '575.txt', '576.txt', '577.txt', '578.txt', '579.txt', '581.txt', '582.txt', '583.txt', '584 (!).txt', '585.txt', '586.txt', '587.txt', '588.txt', '589.txt', '590.txt', '591.txt', '592.txt', '593.txt', '594.txt', '596.txt', '597.txt', '598 (!).txt', '599.txt', '600.txt', '601.txt', '602.txt', '610.txt', '611.txt', '612.txt', '613.txt', '614.txt', '615.txt', '616.txt', '617.txt', '618.txt', '619.txt', '620.txt', '621.txt', '622.txt', '623.txt', '624.txt', '625.txt', '626.txt', '627.txt', '628.txt', '629.txt', '630.txt', '631.txt', '632.txt', '633.txt', 'abdulatipov.txt', 'artjakov.txt', 'Avtovaz.txt', 'blokhin.txt', 'chaves.txt', 'chirkunov.txt', 'kamchatka.txt', 'klinton.txt', 'kuleshov.txt', 'last_01.txt', 'last_02.txt', 'last_03.txt', 'last_04.txt', 'last_05.txt', 'last_06.txt', 'last_07_new.txt', 'last_08.txt', 'last_09.txt', 'last_10.txt', 'last_11.txt', 'last_12.txt', 'last_13.txt', 'last_14.txt', 'last_15.txt', 'last_16.txt', 'last_17.txt', 'last_18.txt', 'last_19.txt', 'last_20.txt', 'last_21.txt', 'last_22.txt', 'last_23.txt', 'last_24.txt', 'last_25.txt', 'last_26.txt', 'last_27.txt', 'last_28.txt', 'last_29.txt', 'last_30_new.txt', 'last_31.txt', 'last_32.txt', 'last_33.txt', 'last_34.txt', 'last_35.txt', 'last_36.txt', 'last_37.txt', 'last_38.txt', 'last_39.txt', 'last_40.txt', 'last_41.txt', 'last_42.txt', 'last_43.txt', 'last_44.txt', 'last_45.txt', 'last_46.txt', 'last_47.txt', 'last_48.txt', 'last_49.txt', 'last_50.txt', 'last_51.txt', 'last_52.txt', 'last_53.txt', 'last_54.txt', 'last_55.txt', 'last_56.txt', 'last_57.txt', 'last_58.txt', 'last_59.txt', 'last_60.txt', 'last_61.txt', 'last_62.txt', 'last_63.txt', 'last_64.txt', 'last_65.txt', 'last_66.txt', 'last_67.txt', 'last_68.txt', 'last_69.txt', 'last_70.txt', 'last_71.txt', 'last_72.txt', 'last_73.txt', 'last_74.txt', 'last_75.txt', 'lenoblast.txt', 'maykl dzhekson.txt', 'mvd.txt', 'mvd2.txt', 'rosobrnadzor.txt', 'ryadovoy chelah.txt', 'semenenko.txt', 'shojgu1.txt', 'shojgu3.txt', 'shojgu4.txt', 'shojgu6.txt', 'si_tzjanpin.txt', 'sobjanin2.txt', 'turkmenija.txt', 'uchitel.txt']\n"
     ]
    }
   ],
   "source": [
    "# Собираем только текстовые файлы коллекции\n",
    "fileDir = r\"Collection5\"\n",
    "fileExt = r\".txt\"\n",
    "documents_txt = [_ for _ in os.listdir(fileDir) if _.endswith(fileExt)]\n",
    "print(documents_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1819111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Россия рассчитывает на конструктивное воздейст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Комиссар СЕ критикует ограничительную политику...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Пулеметы, автоматы и снайперские винтовки изъя...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 октября назначены очередные выборы Верховног...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Следственное управление при прокуратуре требуе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>Депутат от \"ЕР\": К отставке А.Сердюкова причас...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>\\nСи Цзиньпин избран генсеком Коммунистической...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>\"Ведомости\" узнали о смене лидера московских е...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>СМИ узнали о кутежах туркменского чиновника на...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>Вице-мэром Новосибирска по социальным вопросам...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>999 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "0    Россия рассчитывает на конструктивное воздейст...\n",
       "1    Комиссар СЕ критикует ограничительную политику...\n",
       "2    Пулеметы, автоматы и снайперские винтовки изъя...\n",
       "3    4 октября назначены очередные выборы Верховног...\n",
       "4    Следственное управление при прокуратуре требуе...\n",
       "..                                                 ...\n",
       "994  Депутат от \"ЕР\": К отставке А.Сердюкова причас...\n",
       "995  \\nСи Цзиньпин избран генсеком Коммунистической...\n",
       "996  \"Ведомости\" узнали о смене лидера московских е...\n",
       "997  СМИ узнали о кутежах туркменского чиновника на...\n",
       "998  Вице-мэром Новосибирска по социальным вопросам...\n",
       "\n",
       "[999 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Заносим данные файлов txt в датасет\n",
    "text_list = []\n",
    "for file in documents_txt:\n",
    "    doc = open('Collection5/' + file, encoding='utf-8')\n",
    "    text = doc.read()\n",
    "    text_list.append(text)\n",
    "    \n",
    "data_text = pd.DataFrame({'text': text_list })\n",
    "data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c5912ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Россия', 'JJ'),\n",
       " ('рассчитывает', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('конструктивное', 'NNP'),\n",
       " ('воздействие', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('Грузию', 'VBD'),\n",
       " ('04/08/2008', 'CD'),\n",
       " ('12:08', 'CD'),\n",
       " ('МОСКВА', 'NN'),\n",
       " (',', ','),\n",
       " ('4', 'CD'),\n",
       " ('авг', 'SYM'),\n",
       " ('-', ':'),\n",
       " ('РИА', 'NN'),\n",
       " ('Новости', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Россия', 'JJ'),\n",
       " ('рассчитывает', 'NN'),\n",
       " (',', ','),\n",
       " ('что', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('воздействуют', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('Тбилиси', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('связи', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('обострением', 'NNP'),\n",
       " ('ситуации', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('зоне', 'NNP'),\n",
       " ('грузино-осетинского', 'JJ'),\n",
       " ('конфликта', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('Об', 'VB'),\n",
       " ('этом', 'JJ'),\n",
       " ('статс-секретарь', 'JJ'),\n",
       " ('-', ':'),\n",
       " ('заместитель', 'NN'),\n",
       " ('министра', 'JJ'),\n",
       " ('иностранных', 'NNP'),\n",
       " ('дел', 'NNP'),\n",
       " ('России', 'NNP'),\n",
       " ('Григорий', 'NNP'),\n",
       " ('Карасин', 'NNP'),\n",
       " ('заявил', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('телефонном', 'NNP'),\n",
       " ('разговоре', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('заместителем', 'NNP'),\n",
       " ('госсекретаря', 'NNP'),\n",
       " ('США', 'NNP'),\n",
       " ('Дэниэлом', 'NNP'),\n",
       " ('Фридом', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('``', '``'),\n",
       " ('С', 'JJ'),\n",
       " ('российской', 'NN'),\n",
       " ('стороны', 'NNP'),\n",
       " ('выражена', 'NNP'),\n",
       " ('глубокая', 'NNP'),\n",
       " ('озабоченность', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('связи', 'NNP'),\n",
       " ('с', 'NNP'),\n",
       " ('новым', 'NNP'),\n",
       " ('витком', 'NNP'),\n",
       " ('напряженности', 'NNP'),\n",
       " ('вокруг', 'NNP'),\n",
       " ('Южной', 'NNP'),\n",
       " ('Осетии', 'NNP'),\n",
       " (',', ','),\n",
       " ('противозаконными', 'NNP'),\n",
       " ('действиями', 'NNP'),\n",
       " ('грузинской', 'NNP'),\n",
       " ('стороны', 'NNP'),\n",
       " ('по', 'NNP'),\n",
       " ('наращиванию', 'NNP'),\n",
       " ('своих', 'NNP'),\n",
       " ('вооруженных', 'NNP'),\n",
       " ('сил', 'NNP'),\n",
       " ('в', 'NNP'),\n",
       " ('регионе', 'NNP'),\n",
       " (',', ','),\n",
       " ('бесконтрольным', 'NNP'),\n",
       " ('строительством', 'NNP'),\n",
       " ('фортификационных', 'NNP'),\n",
       " ('сооружений', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " (',', ','),\n",
       " ('-', ':'),\n",
       " ('говорится', 'NN'),\n",
       " ('в', 'JJ'),\n",
       " ('сообщении', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('``', '``'),\n",
       " ('Россия', 'JJ'),\n",
       " ('уже', 'NN'),\n",
       " ('призвала', 'NNP'),\n",
       " ('Тбилиси', 'NNP'),\n",
       " ('к', 'NNP'),\n",
       " ('ответственной', 'NNP'),\n",
       " ('линии', 'NNP'),\n",
       " ('и', 'NNP'),\n",
       " ('рассчитывает', 'NNP'),\n",
       " ('также', 'NNP'),\n",
       " ('на', 'NNP'),\n",
       " ('конструктивное', 'NNP'),\n",
       " ('воздействие', 'NNP'),\n",
       " ('со', 'NNP'),\n",
       " ('стороны', 'NNP'),\n",
       " ('Вашингтона', 'NNP'),\n",
       " (\"''\", \"''\"),\n",
       " (',', ','),\n",
       " ('-', ':'),\n",
       " ('сообщил', 'NN'),\n",
       " ('МИД', 'JJ'),\n",
       " ('России', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример текста\n",
    "document = data_text.text[0]\n",
    "\n",
    "# Разбиваем документ на токены и применяем pos tagging (на выходе список кортежей (токен, часть речи))\n",
    "nltk.pos_tag(nltk.word_tokenize(document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4d973379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('МИД России', 'ORGANIZATION'),\n",
       " ('МОСКВА', 'ORGANIZATION'),\n",
       " ('РИА Новости', 'ORGANIZATION'),\n",
       " ('России Григорий Карасин', 'PERSON'),\n",
       " ('Россия', 'PERSON'),\n",
       " ('Тбилиси', 'PERSON')}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Распознаем именнованные сущности с помощью классификатора (Person, Organization, GPE)\n",
    "{(' '.join(c[0] for c in chunk), chunk.label()) for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(document))) if hasattr(chunk, 'label')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53c6b460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>PER 20 27</th>\n",
       "      <th>Ассанжу</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T2</td>\n",
       "      <td>PER 51 67</td>\n",
       "      <td>Джулиана Ассанжа</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T3</td>\n",
       "      <td>PER 220 226</td>\n",
       "      <td>Ассанж</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T4</td>\n",
       "      <td>ORG 514 523</td>\n",
       "      <td>WikiLeaks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5</td>\n",
       "      <td>PER 649 658</td>\n",
       "      <td>Д. Ассанж</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T6</td>\n",
       "      <td>PER 1038 1047</td>\n",
       "      <td>Д. Ассанж</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>T124</td>\n",
       "      <td>GEOPOLIT 11243 11249</td>\n",
       "      <td>Швеции</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>T125</td>\n",
       "      <td>PER 11349 11358</td>\n",
       "      <td>Д. Ассанж</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>T126</td>\n",
       "      <td>PER 11473 11483</td>\n",
       "      <td>Д. Ассанжа</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>T127</td>\n",
       "      <td>GEOPOLIT 11849 11857</td>\n",
       "      <td>Эквадора</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>T128</td>\n",
       "      <td>LOC 11860 11867</td>\n",
       "      <td>Лондоне</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>127 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       T1             PER 20 27           Ассанжу\n",
       "0      T2             PER 51 67  Джулиана Ассанжа\n",
       "1      T3           PER 220 226            Ассанж\n",
       "2      T4           ORG 514 523         WikiLeaks\n",
       "3      T5           PER 649 658         Д. Ассанж\n",
       "4      T6         PER 1038 1047         Д. Ассанж\n",
       "..    ...                   ...               ...\n",
       "122  T124  GEOPOLIT 11243 11249            Швеции\n",
       "123  T125       PER 11349 11358         Д. Ассанж\n",
       "124  T126       PER 11473 11483        Д. Ассанжа\n",
       "125  T127  GEOPOLIT 11849 11857          Эквадора\n",
       "126  T128       LOC 11860 11867           Лондоне\n",
       "\n",
       "[127 rows x 3 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Разметка из Collection5\n",
    "pd.read_csv('Collection5/2003.ann', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816076d3",
   "metadata": {},
   "source": [
    "#### SPACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4665db6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "ny_bb = data_text.text[1]\n",
    "article = nlp(ny_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "684ea9c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Комиссар \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    СЕ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " критикует ограничительную политику в отношении беженцев в европейских странах</br></br>05/08/2008 10:32</br></br>МОСКВА, 5 августа /Новости-Грузия/.  Проводимая в европейских странах ограничительная политика в отношении беженцев нарушает ряд международных стандартов, в частности, право на воссоединение семей, заявляет Комиссар \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Совета Европы по правам человека\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Томас Хаммарберг (Thomas Hammarberg)\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " в размещенном на его сайте еженедельном комментарии.</br></br>&quot;Ограничительная политика в отношении беженцев в европейских странах уменьшает возможности воссоединения разделенных семей&quot;, - полагает он.</br></br>По сообщению \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    РИА Новости\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Хаммарберг\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " констатирует, что в последнее время &quot;правительства попытались ограничить приезд близких родственников к тем беженцам, которые уже проживают в стране&quot;.</br></br>Комиссар не называет конкретных стран, одновременно отмечая, что в ряде случаев подобная линия привела &quot;к неоправданным человеческим страданиям, когда члены семьи, зависящие друг от друга, оказались разделенными&quot;.</br></br>&quot;Такая политика противоречит праву на воссоединение семей, как это предусмотрено некоторыми международными стандартами&quot;, - замечает он.</br></br>Комиссар \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Совета Европы\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " призывает страны учитывать в политике, проводимой в отношении беженцев, положения о семье, принятые в рамках \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ООН\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " и \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ЕС\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(article, jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62ff6cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Комиссар NOUN nsubj\n",
      "СЕ PROPN nmod\n",
      "критикует VERB ROOT\n",
      "ограничительную ADJ amod\n",
      "политику NOUN obj\n",
      "в ADP case\n",
      "отношении NOUN fixed\n",
      "беженцев NOUN nmod\n",
      "в ADP case\n",
      "европейских ADJ amod\n",
      "странах NOUN nmod\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "05/08/2008 NUM appos\n",
      "10:32 NUM appos\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "МОСКВА PROPN obl\n",
      ", PUNCT punct\n",
      "5 ADJ obl\n",
      "августа NOUN flat\n",
      "/Новости NOUN flat\n",
      "- NOUN nsubj\n",
      "Грузия/. PROPN nsubj\n",
      "  SPACE dep\n",
      "Проводимая VERB acl\n",
      "в ADP case\n",
      "европейских ADJ amod\n",
      "странах NOUN obl\n",
      "ограничительная ADJ amod\n",
      "политика NOUN nsubj\n",
      "в ADP case\n",
      "отношении NOUN fixed\n",
      "беженцев NOUN nmod\n",
      "нарушает VERB conj\n",
      "ряд NOUN obj\n",
      "международных ADJ amod\n",
      "стандартов NOUN nmod\n",
      ", PUNCT punct\n",
      "в ADP discourse\n",
      "частности NOUN fixed\n",
      ", PUNCT punct\n",
      "право NOUN parataxis\n",
      "на ADP case\n",
      "воссоединение NOUN nmod\n",
      "семей NOUN nmod\n",
      ", PUNCT punct\n",
      "заявляет VERB parataxis\n",
      "Комиссар PROPN nsubj\n",
      "Совета PROPN nmod\n",
      "Европы PROPN nmod\n",
      "по ADP case\n",
      "правам NOUN nmod\n",
      "человека NOUN nmod\n",
      "Томас PROPN appos\n",
      "Хаммарберг PROPN flat:name\n",
      "( PUNCT punct\n",
      "Thomas PROPN appos\n",
      "Hammarberg PROPN flat:name\n",
      ") PUNCT punct\n",
      "в ADP case\n",
      "размещенном VERB acl\n",
      "на ADP case\n",
      "его DET det\n",
      "сайте NOUN obl\n",
      "еженедельном ADJ amod\n",
      "комментарии NOUN obl\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "\" PUNCT punct\n",
      "Ограничительная ADJ amod\n",
      "политика NOUN nsubj\n",
      "в ADP case\n",
      "отношении NOUN fixed\n",
      "беженцев NOUN nmod\n",
      "в ADP case\n",
      "европейских ADJ amod\n",
      "странах NOUN obl\n",
      "уменьшает VERB ROOT\n",
      "возможности NOUN obj\n",
      "воссоединения NOUN nmod\n",
      "разделенных ADJ amod\n",
      "семей NOUN nmod\n",
      "\" PUNCT punct\n",
      ", PUNCT punct\n",
      "- PUNCT punct\n",
      "полагает VERB parataxis\n",
      "он PRON nsubj\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "По ADP case\n",
      "сообщению NOUN parataxis\n",
      "РИА PROPN nmod\n",
      "Новости PROPN appos\n",
      ", PUNCT punct\n",
      "Хаммарберг PROPN nsubj\n",
      "констатирует VERB ROOT\n",
      ", PUNCT punct\n",
      "что SCONJ mark\n",
      "в ADP case\n",
      "последнее ADJ amod\n",
      "время NOUN obl\n",
      "\" PUNCT punct\n",
      "правительства NOUN nsubj\n",
      "попытались VERB ccomp\n",
      "ограничить VERB xcomp\n",
      "приезд NOUN obj\n",
      "близких ADJ amod\n",
      "родственников NOUN nmod\n",
      "к ADP case\n",
      "тем DET det\n",
      "беженцам NOUN nmod\n",
      ", PUNCT punct\n",
      "которые PRON nsubj\n",
      "уже ADV advmod\n",
      "проживают VERB acl:relcl\n",
      "в ADP case\n",
      "стране NOUN obl\n",
      "\" PUNCT punct\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "Комиссар NOUN nsubj\n",
      "не PART advmod\n",
      "называет VERB ROOT\n",
      "конкретных ADJ amod\n",
      "стран NOUN obj\n",
      ", PUNCT punct\n",
      "одновременно ADV advmod\n",
      "отмечая VERB advcl\n",
      ", PUNCT punct\n",
      "что SCONJ mark\n",
      "в ADP case\n",
      "ряде NOUN obl\n",
      "случаев NOUN nmod\n",
      "подобная ADJ amod\n",
      "линия NOUN nsubj\n",
      "привела VERB ccomp\n",
      "\" PUNCT punct\n",
      "к ADP case\n",
      "неоправданным ADJ amod\n",
      "человеческим ADJ amod\n",
      "страданиям NOUN obl\n",
      ", PUNCT punct\n",
      "когда SCONJ mark\n",
      "члены NOUN nsubj\n",
      "семьи NOUN nmod\n",
      ", PUNCT punct\n",
      "зависящие VERB acl\n",
      "друг PRON obl\n",
      "от ADP fixed\n",
      "друга PRON fixed\n",
      ", PUNCT punct\n",
      "оказались VERB advcl\n",
      "разделенными ADJ xcomp\n",
      "\" PUNCT punct\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "\" PUNCT punct\n",
      "Такая DET det\n",
      "политика NOUN nsubj\n",
      "противоречит VERB ROOT\n",
      "праву NOUN iobj\n",
      "на ADP case\n",
      "воссоединение NOUN obl\n",
      "семей NOUN nmod\n",
      ", PUNCT punct\n",
      "как SCONJ mark\n",
      "это PRON nsubj:pass\n",
      "предусмотрено VERB acl\n",
      "некоторыми DET det\n",
      "международными ADJ amod\n",
      "стандартами NOUN obl\n",
      "\" PUNCT punct\n",
      ", PUNCT punct\n",
      "- PUNCT punct\n",
      "замечает VERB parataxis\n",
      "он PRON nsubj\n",
      ". PUNCT punct\n",
      "\n",
      "\n",
      " SPACE dep\n",
      "Комиссар NOUN nsubj\n",
      "Совета PROPN nmod\n",
      "Европы PROPN nmod\n",
      "призывает VERB ROOT\n",
      "страны NOUN obj\n",
      "учитывать VERB xcomp\n",
      "в ADP case\n",
      "политике NOUN obl\n",
      ", PUNCT punct\n",
      "проводимой VERB acl\n",
      "в ADP case\n",
      "отношении NOUN fixed\n",
      "беженцев NOUN obl\n",
      ", PUNCT punct\n",
      "положения NOUN obj\n",
      "о ADP case\n",
      "семье NOUN nmod\n",
      ", PUNCT punct\n",
      "принятые VERB acl\n",
      "в ADP case\n",
      "рамках NOUN obl\n",
      "ООН PROPN nmod\n",
      "и CCONJ cc\n",
      "ЕС PROPN conj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "for token in article:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7cbbbe",
   "metadata": {},
   "source": [
    "#### Deeppavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde2b10f",
   "metadata": {},
   "source": [
    "###### pip install pytorch-crf==0.4.0 needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0c5e42f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 18:58:36.358 INFO in 'deeppavlov.core.data.utils'['utils'] at line 95: Downloading from http://files.deeppavlov.ai/v1/ner/ner_rus_bert_torch_new.tar.gz to C:\\Users\\xiaomi\\.deeppavlov\\models\\ner_rus_bert_torch_new.tar.gz\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1.44G/1.44G [10:45<00:00, 2.23MB/s]\n",
      "2023-07-18 19:09:22.373 INFO in 'deeppavlov.core.data.utils'['utils'] at line 276: Extracting C:\\Users\\xiaomi\\.deeppavlov\\models\\ner_rus_bert_torch_new.tar.gz archive into C:\\Users\\xiaomi\\.deeppavlov\\models\\ner_rus_bert_torch\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e677f45b920e49e0ba50108f91f7ef4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2023-07-18 19:16:04.111 ERROR in 'deeppavlov.core.common.params'['params'] at line 108: Exception in <class 'deeppavlov.models.torch_bert.torch_transformers_sequence_tagger.TorchTransformersSequenceTagger'>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xiaomi\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\core\\common\\params.py\", line 102, in from_params\n",
      "    component = obj(**dict(config_params, **kwargs))\n",
      "  File \"C:\\Users\\xiaomi\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\models\\torch_bert\\torch_transformers_sequence_tagger.py\", line 181, in __init__\n",
      "    super().__init__(optimizer=optimizer,\n",
      "  File \"C:\\Users\\xiaomi\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\core\\models\\torch_model.py\", line 97, in __init__\n",
      "    self.load()\n",
      "  File \"C:\\Users\\xiaomi\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\models\\torch_bert\\torch_transformers_sequence_tagger.py\", line 302, in load\n",
      "    super().load()\n",
      "  File \"C:\\Users\\xiaomi\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\core\\models\\torch_model.py\", line 167, in load\n",
      "    checkpoint = torch.load(weights_path, map_location=self.device)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 607, in load\n",
      "    return _load(opened_zipfile, map_location, pickle_module, **pickle_load_args)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 882, in _load\n",
      "    result = unpickler.load()\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 857, in persistent_load\n",
      "    load_tensor(data_type, size, key, _maybe_decode_ascii(location))\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 846, in load_tensor\n",
      "    loaded_storages[key] = restore_location(storage, location)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 827, in restore_location\n",
      "    return default_restore_location(storage, str(map_location))\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 175, in default_restore_location\n",
      "    result = fn(storage, location)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\", line 157, in _cuda_deserialize\n",
      "    return obj.cuda(device)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_utils.py\", line 79, in _cuda\n",
      "    return new_type(self.size()).copy_(self, non_blocking)\n",
      "  File \"D:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 528, in _lazy_new\n",
      "    return super(_CudaBase, cls).__new__(cls, *args, **kwargs)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 2.00 GiB total capacity; 1.66 GiB already allocated; 0 bytes free; 1.74 GiB reserved in total by PyTorch)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 2.00 GiB total capacity; 1.66 GiB already allocated; 0 bytes free; 1.74 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23320\\912612023.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeeppavlov\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdeeppavlov\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdeeppavlov_ner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mner_rus_bert\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mrus_document\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdeeppavlov_ner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrus_document\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\core\\commands\\infer.py\u001b[0m in \u001b[0;36mbuild_model\u001b[1;34m(config, mode, load_trained, install, download)\u001b[0m\n\u001b[0;32m     53\u001b[0m                             .format(component_config.get('class_name', component_config.get('ref', 'UNKNOWN'))))\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrom_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomponent_config\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'id'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcomponent_config\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\core\\common\\params.py\u001b[0m in \u001b[0;36mfrom_params\u001b[1;34m(params, mode, **kwargs)\u001b[0m\n\u001b[0;32m    100\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mode'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m             \u001b[0mcomponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m                 \u001b[0m_refs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconfig_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\models\\torch_bert\\torch_transformers_sequence_tagger.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_tags, pretrained_bert, bert_config_file, attention_probs_keep_prob, hidden_keep_prob, optimizer, optimizer_parameters, learning_rate_drop_patience, learning_rate_drop_div, load_before_drop, clip_norm, min_learning_rate, use_crf, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_crf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muse_crf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m         super().__init__(optimizer=optimizer,\n\u001b[0m\u001b[0;32m    182\u001b[0m                          \u001b[0moptimizer_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer_parameters\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m                          \u001b[0mlearning_rate_drop_patience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate_drop_patience\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\core\\models\\torch_model.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, device, optimizer, optimizer_parameters, lr_scheduler, lr_scheduler_parameters, learning_rate_drop_patience, learning_rate_drop_div, load_before_drop, min_learning_rate, *args, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[1;31m# we need to switch to eval mode here because by default it's in `train` mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;31m# But in case of `interact/build_model` usage, we need to have model in eval mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\models\\torch_bert\\torch_transformers_sequence_tagger.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, fname)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m             \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_crf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m                 \u001b[0mweights_path_crf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.load_path}_crf\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\deeppavlov\\core\\models\\torch_model.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, fname, *args, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[1;31m# now load the weights, optimizer from saved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Loading weights from {weights_path}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m                 \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m                 \u001b[0mmodel_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model_state_dict\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[0moptimizer_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"optimizer_state_dict\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    605\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 607\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    608\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m    880\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    881\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 882\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m    855\u001b[0m         \u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 857\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    858\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(data_type, size, key, location)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 846\u001b[1;33m         \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    847\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    848\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mrestore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    825\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 827\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    828\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mrestore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mdefault_restore_location\u001b[1;34m(storage, location)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdefault_restore_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_package_registry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_cuda_deserialize\u001b[1;34m(obj, location)\u001b[0m\n\u001b[0;32m    155\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mstorage_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_utils.py\u001b[0m in \u001b[0;36m_cuda\u001b[1;34m(self, device, non_blocking, **kwargs)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[0mnew_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_new\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m     \u001b[1;31m# We may need to call lazy init again if we are a forked child\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m     \u001b[1;31m# del _CudaBase.__new__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_CudaBase\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__new__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 352.00 MiB (GPU 0; 2.00 GiB total capacity; 1.66 GiB already allocated; 0 bytes free; 1.74 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import deeppavlov\n",
    "from deeppavlov import configs, build_model\n",
    "deeppavlov_ner = build_model(configs.ner.ner_rus_bert, download=True)\n",
    "rus_document = \"Нью-Йорк, США, 30 апреля 2020, 01:01 — REGNUM В администрации президента США Дональда Трампа планируют пройти все этапы создания вакцины от коронавируса в ускоренном темпе и выпустить 100 млн доз до конца 2020 года, передаёт агентство Bloomberg со ссылкой на осведомлённые источники\"\n",
    "deeppavlov_ner([rus_document])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a3463b",
   "metadata": {},
   "source": [
    "### К сожалению не хватает памяти(("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e2de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['001.ann', '002.ann', '003.ann', '004.ann', '005.ann', '006.ann', '007.ann', '008.ann', '009.ann', '010.ann', '011.ann', '012.ann', '013.ann', '014.ann', '015 (!).ann', '016.ann', '017.ann', '018.ann', '019.ann', '020.ann', '021.ann', '022.ann', '023.ann', '025.ann', '026.ann', '027.ann', '028.ann', '029.ann', '030.ann', '031.ann', '032.ann', '033.ann', '034.ann', '035.ann', '036.ann', '037.ann', '038.ann', '039.ann', '03_12_12a.ann', '03_12_12b.ann', '03_12_12c.ann', '03_12_12d.ann', '03_12_12g.ann', '03_12_12h.ann', '040.ann', '041.ann', '042.ann', '043.ann', '044.ann', '045.ann', '046.ann', '047.ann', '048.ann', '049.ann', '04_02_13a_abdulatipov.ann', '04_03_13a_sorokin.ann', '04_12_12b.ann', '04_12_12d.ann', '04_12_12f.ann', '04_12_12g.ann', '04_12_12h_corr.ann', '050.ann', '051.ann', '052.ann', '053.ann', '054.ann', '055.ann', '056.ann', '057.ann', '058.ann', '059.ann', '060.ann', '061.ann', '062.ann', '063.ann', '064.ann', '065.ann', '066.ann', '067.ann', '068.ann', '069.ann', '070.ann', '071.ann', '072.ann', '073.ann', '074.ann', '075.ann', '076.ann', '077.ann', '078.ann', '079.ann', '080.ann', '081.ann', '082.ann', '083.ann', '084.ann', '085.ann', '086.ann', '087.ann', '088.ann', '089.ann', '090.ann', '091.ann', '092.ann', '093.ann', '094.ann', '095.ann', '096.ann', '097.ann', '098.ann', '099.ann', '09_01_13.ann', '09_01_13a.ann', '09_01_13c.ann', '09_01_13d.ann', '09_01_13e.ann', '09_01_13h.ann', '09_01_13i.ann', '100.ann', '1000.ann', '1001.ann', '1002.ann', '1003.ann', '1004.ann', '1005.ann', '1006.ann', '1007.ann', '1008.ann', '1009.ann', '101.ann', '1010.ann', '1011.ann', '1012.ann', '1013.ann', '1014.ann', '1015.ann', '1016.ann', '1017.ann', '1018.ann', '1019.ann', '102.ann', '1020.ann', '1021.ann', '1022.ann', '1023.ann', '1024.ann', '1025.ann', '1026.ann', '1027.ann', '1028.ann', '1029.ann', '103.ann', '1030.ann', '1031.ann', '1032.ann', '1033.ann', '1034.ann', '1035.ann', '1036.ann', '1037.ann', '1038.ann', '1039.ann', '104.ann', '1040.ann', '1041.ann', '1042.ann', '1043.ann', '1044.ann', '1045.ann', '1046.ann', '1047.ann', '1048.ann', '1049.ann', '105.ann', '1050.ann', '106.ann', '107.ann', '108.ann', '109.ann', '10_01_13a.ann', '10_01_13d.ann', '10_01_13i.ann', '110.ann', '1100.ann', '1101.ann', '1102.ann', '1103.ann', '1104.ann', '1105.ann', '1106.ann', '1107.ann', '1108.ann', '1109.ann', '111.ann', '1110.ann', '1111.ann', '1112.ann', '1113.ann', '1114.ann', '1115.ann', '1116.ann', '1117.ann', '1118.ann', '1119.ann', '112.ann', '1120.ann', '1121.ann', '1122.ann', '1123.ann', '1124.ann', '1125.ann', '1126.ann', '1127.ann', '1128.ann', '113.ann', '1130.ann', '1131.ann', '1132.ann', '1133.ann', '1134.ann', '1135.ann', '1136.ann', '1137.ann', '1138.ann', '1139.ann', '114.ann', '1140.ann', '1141.ann', '1142.ann', '1143.ann', '1144.ann', '1145.ann', '1146.ann', '1147.ann', '1148.ann', '1149.ann', '115.ann', '1150.ann', '1151.ann', '1152.ann', '1153.ann', '1154.ann', '1155.ann', '1156.ann', '1157.ann', '1158.ann', '1159.ann', '116.ann', '1160.ann', '1161.ann', '1162.ann', '1163.ann', '1164.ann', '1165.ann', '1166.ann', '1167.ann', '1168.ann', '1169.ann', '117.ann', '1170.ann', '1171.ann', '1172.ann', '1173.ann', '1174.ann', '1175.ann', '1176.ann', '1177.ann', '1178.ann', '1179.ann', '118.ann', '1180.ann', '1181.ann', '1182.ann', '1183.ann', '1184.ann', '1185.ann', '1186.ann', '1187.ann', '1188.ann', '1189.ann', '119.ann', '1190.ann', '1191.ann', '1192.ann', '1193.ann', '1194.ann', '1195.ann', '1196.ann', '1197.ann', '1198.ann', '1199.ann', '11_01_13b.ann', '11_01_13e.ann', '120.ann', '1200.ann', '121.ann', '122.ann', '123.ann', '124.ann', '125.ann', '126.ann', '127.ann', '128.ann', '129.ann', '130.ann', '131.ann', '132.ann', '133.ann', '134.ann', '135.ann', '136.ann', '137.ann', '138.ann', '139.ann', '140.ann', '141.ann', '142.ann', '143.ann', '144.ann', '145.ann', '146.ann', '147.ann', '148.ann', '149.ann', '14_01_13c.ann', '14_01_13g.ann', '14_01_13i.ann', '150.ann', '151.ann', '152.ann', '153.ann', '154.ann', '155.ann', '156.ann', '157.ann', '158.ann', '159.ann', '15_01_13a.ann', '15_01_13b.ann', '15_01_13e.ann', '15_01_13f.ann', '160.ann', '161.ann', '162.ann', '163.ann', '164.ann', '165.ann', '166.ann', '167.ann', '168.ann', '169.ann', '170.ann', '171.ann', '172.ann', '173.ann', '174.ann', '175.ann', '176.ann', '177.ann', '178.ann', '179.ann', '180.ann', '181.ann', '182.ann', '183.ann', '184.ann', '185.ann', '186.ann', '187.ann', '188.ann', '189.ann', '190.ann', '191.ann', '192.ann', '193.ann', '194.ann', '195.ann', '196.ann', '197.ann', '198.ann', '199.ann', '19_11_12d.ann', '19_11_12h.ann', '200.ann', '2001.ann', '2002.ann', '2003.ann', '2004.ann', '2005.ann', '2006.ann', '2007.ann', '2008.ann', '2009.ann', '201.ann', '2010.ann', '2011.ann', '2012.ann', '2013.ann', '2014.ann', '2015.ann', '2016.ann', '2017.ann', '2018.ann', '2019.ann', '202.ann', '2020.ann', '2021.ann', '2022.ann', '2023.ann', '2024.ann', '2025.ann', '2026.ann', '2027.ann', '2028.ann', '2029.ann', '203.ann', '2030.ann', '2031.ann', '2032.ann', '2034.ann', '2035.ann', '2036.ann', '2037.ann', '2038.ann', '2039.ann', '204.ann', '2040.ann', '2041.ann', '2042.ann', '2043.ann', '2044.ann', '2045.ann', '2046.ann', '2047.ann', '2048.ann', '2049.ann', '205.ann', '2050.ann', '206.ann', '207.ann', '208.ann', '209.ann', '20_11_12a.ann', '20_11_12b.ann', '20_11_12c.ann', '20_11_12d.ann', '20_11_12i.ann', '210.ann', '211.ann', '212.ann', '213.ann', '214.ann', '215.ann', '216.ann', '217.ann', '218.ann', '219.ann', '21_11_12c.ann', '21_11_12h.ann', '21_11_12i.ann', '21_11_12j.ann', '220.ann', '221.ann', '222.ann', '223.ann', '224.ann', '225.ann', '226.ann', '227.ann', '228.ann', '229.ann', '22_11_12a.ann', '22_11_12c.ann', '22_11_12d.ann', '22_11_12g.ann', '22_11_12h.ann', '22_11_12i.ann', '22_11_12j.ann', '230.ann', '231.ann', '232.ann', '233.ann', '234.ann', '235.ann', '236.ann', '237.ann', '238.ann', '239.ann', '23_11_12a.ann', '23_11_12b.ann', '23_11_12c.ann', '23_11_12d.ann', '23_11_12e.ann', '23_11_12f.ann', '240.ann', '241.ann', '242.ann', '243.ann', '244.ann', '245.ann', '246.ann', '247.ann', '248.ann', '249.ann', '250.ann', '251.ann', '252.ann', '253.ann', '254.ann', '255.ann', '256.ann', '257.ann', '258.ann', '259.ann', '25_12_12a.ann', '25_12_12c.ann', '25_12_12d.ann', '25_12_12e.ann', '260.ann', '261.ann', '262.ann', '263.ann', '264.ann', '265.ann', '266.ann', '267.ann', '268.ann', '269.ann', '26_11_12b.ann', '26_11_12c.ann', '26_11_12e.ann', '26_11_12f.ann', '270.ann', '271.ann', '272.ann', '273.ann', '274.ann', '275.ann', '276.ann', '277.ann', '278.ann', '279.ann', '27_11_12a.ann', '27_11_12c.ann', '27_11_12d.ann', '27_11_12e.ann', '27_11_12j.ann', '280.ann', '281.ann', '282.ann', '283.ann', '284.ann', '285.ann', '286.ann', '287.ann', '288.ann', '289.ann', '28_11_12a.ann', '28_11_12f.ann', '28_11_12g.ann', '28_11_12h.ann', '28_11_12i.ann', '28_11_12j.ann', '290.ann', '291.ann', '292.ann', '293.ann', '294.ann', '295.ann', '296.ann', '297.ann', '298.ann', '299.ann', '29_11_12a.ann', '29_11_12b.ann', '300.ann', '301.ann', '302.ann', '303.ann', '304.ann', '305.ann', '306.ann', '307.ann', '308.ann', '309.ann', '30_11_12b.ann', '30_11_12h.ann', '30_11_12i.ann', '310.ann', '311.ann', '312.ann', '313.ann', '314.ann', '315.ann', '316.ann', '317.ann', '318.ann', '319.ann', '320.ann', '321.ann', '322.ann', '323.ann', '324.ann', '325.ann', '326.ann', '327.ann', '328.ann', '329.ann', '330.ann', '331.ann', '332.ann', '333.ann', '334.ann', '335.ann', '336.ann', '337.ann', '338.ann', '339.ann', '340.ann', '341.ann', '342.ann', '343.ann', '344.ann', '345.ann', '346.ann', '347.ann', '348.ann', '349.ann', '350.ann', '351.ann', '352.ann', '353.ann', '354.ann', '355.ann', '356.ann', '357.ann', '358.ann', '359.ann', '360.ann', '361.ann', '362.ann', '363.ann', '364.ann', '365.ann', '366.ann', '367.ann', '368.ann', '369.ann', '370.ann', '371.ann', '372.ann', '373.ann', '374.ann', '375.ann', '376.ann', '377.ann', '378.ann', '379.ann', '380.ann', '381.ann', '382.ann', '383.ann', '384.ann', '385.ann', '386.ann', '387.ann', '388.ann', '389.ann', '390.ann', '391.ann', '392.ann', '393.ann', '394.ann', '395.ann', '396.ann', '397.ann', '398.ann', '399.ann', '400.ann', '401.ann', '402.ann', '403.ann', '404.ann', '405.ann', '406.ann', '407.ann', '408.ann', '409.ann', '410.ann', '411.ann', '412.ann', '413.ann', '414.ann', '415.ann', '416.ann', '417.ann', '418.ann', '419.ann', '420.ann', '421.ann', '422.ann', '423.ann', '424.ann', '425.ann', '426.ann', '427.ann', '428.ann', '429.ann', '430.ann', '431.ann', '432.ann', '433.ann', '434.ann', '435.ann', '436.ann', '437.ann', '438.ann', '439.ann', '440.ann', '441.ann', '442.ann', '443.ann', '444.ann', '445.ann', '446.ann', '447.ann', '448.ann', '449.ann', '450.ann', '451.ann', '452.ann', '453.ann', '454.ann', '455.ann', '457.ann', '458.ann', '459.ann', '460.ann', '461.ann', '462.ann', '463.ann', '464.ann', '465.ann', '466.ann', '467.ann', '468.ann', '469.ann', '470.ann', '471.ann', '472.ann', '473.ann', '474.ann', '475.ann', '476.ann', '477.ann', '478.ann', '479.ann', '480.ann', '481.ann', '482.ann', '483.ann', '484.ann', '485.ann', '486.ann', '487.ann', '488.ann', '489.ann', '490.ann', '491.ann', '492.ann', '493.ann', '494.ann', '495.ann', '496.ann', '497.ann', '498.ann', '499.ann', '500.ann', '501.ann', '502.ann', '503.ann', '504.ann', '505.ann', '506.ann', '507.ann', '508.ann', '509.ann', '510.ann', '511.ann', '512.ann', '513.ann', '514.ann', '515.ann', '516.ann', '517.ann', '518.ann', '519.ann', '520.ann', '521.ann', '522.ann', '523.ann', '524.ann', '525.ann', '526.ann', '527.ann', '528.ann', '529.ann', '530.ann', '531.ann', '532.ann', '533 (!).ann', '534.ann', '535.ann', '536.ann', '537.ann', '538.ann', '539.ann', '540.ann', '541.ann', '542.ann', '543.ann', '544.ann', '545.ann', '546.ann', '547.ann', '548.ann', '549.ann', '550.ann', '551.ann', '552.ann', '553.ann', '554.ann', '555 (!).ann', '556.ann', '557.ann', '558.ann', '559.ann', '560.ann', '561.ann', '562.ann', '563.ann', '564.ann', '565.ann', '567.ann', '568.ann', '569.ann', '570.ann', '571.ann', '572.ann', '574.ann', '575.ann', '576.ann', '577.ann', '578.ann', '579.ann', '581.ann', '582.ann', '583.ann', '584 (!).ann', '585.ann', '586.ann', '587.ann', '588.ann', '589.ann', '590.ann', '591.ann', '592.ann', '593.ann', '594.ann', '596.ann', '597.ann', '598 (!).ann', '599.ann', '600.ann', '601.ann', '602.ann', '610.ann', '611.ann', '612.ann', '613.ann', '614.ann', '615.ann', '616.ann', '617.ann', '618.ann', '619.ann', '620.ann', '621.ann', '622.ann', '623.ann', '624.ann', '625.ann', '626.ann', '627.ann', '628.ann', '629.ann', '630.ann', '631.ann', '632.ann', '633.ann', 'abdulatipov.ann', 'artjakov.ann', 'Avtovaz.ann', 'blokhin.ann', 'chaves.ann', 'chirkunov.ann', 'kamchatka.ann', 'klinton.ann', 'kuleshov.ann', 'last_01.ann', 'last_02.ann', 'last_03.ann', 'last_04.ann', 'last_05.ann', 'last_06.ann', 'last_07_new.ann', 'last_08.ann', 'last_09.ann', 'last_10.ann', 'last_11.ann', 'last_12.ann', 'last_13.ann', 'last_14.ann', 'last_15.ann', 'last_16.ann', 'last_17.ann', 'last_18.ann', 'last_19.ann', 'last_20.ann', 'last_21.ann', 'last_22.ann', 'last_23.ann', 'last_24.ann', 'last_25.ann', 'last_26.ann', 'last_27.ann', 'last_28.ann', 'last_29.ann', 'last_30_new.ann', 'last_31.ann', 'last_32.ann', 'last_33.ann', 'last_34.ann', 'last_35.ann', 'last_36.ann', 'last_37.ann', 'last_38.ann', 'last_39.ann', 'last_40.ann', 'last_41.ann', 'last_42.ann', 'last_43.ann', 'last_44.ann', 'last_45.ann', 'last_46.ann', 'last_47.ann', 'last_48.ann', 'last_49.ann', 'last_50.ann', 'last_51.ann', 'last_52.ann', 'last_53.ann', 'last_54.ann', 'last_55.ann', 'last_56.ann', 'last_57.ann', 'last_58.ann', 'last_59.ann', 'last_60.ann', 'last_61.ann', 'last_62.ann', 'last_63.ann', 'last_64.ann', 'last_65.ann', 'last_66.ann', 'last_67.ann', 'last_68.ann', 'last_69.ann', 'last_70.ann', 'last_71.ann', 'last_72.ann', 'last_73.ann', 'last_74.ann', 'last_75.ann', 'lenoblast.ann', 'maykl dzhekson.ann', 'mvd.ann', 'mvd2.ann', 'rosobrnadzor.ann', 'ryadovoy chelah.ann', 'semenenko.ann', 'shojgu1.ann', 'shojgu3.ann', 'shojgu4.ann', 'shojgu6.ann', 'si_tzjanpin.ann', 'sobjanin2.ann', 'turkmenija.ann', 'uchitel.ann']\n"
     ]
    }
   ],
   "source": [
    "# Собираем только ann файлы коллекции\n",
    "fileDir = r\"Collection5\"\n",
    "fileExt = r\".ann\"\n",
    "documents_ann = [_ for _ in os.listdir(fileDir) if _.endswith(fileExt)]\n",
    "print(documents_ann)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f71718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T1</td>\n",
       "      <td>LOC 82 89</td>\n",
       "      <td>Бишкеке</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T2</td>\n",
       "      <td>LOC 113 119</td>\n",
       "      <td>БИШКЕК</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T3</td>\n",
       "      <td>MEDIA 132 146</td>\n",
       "      <td>Новости-Грузия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T4</td>\n",
       "      <td>GEOPOLIT 175 183</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5</td>\n",
       "      <td>GEOPOLIT 225 228</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T6</td>\n",
       "      <td>LOC 231 238</td>\n",
       "      <td>Бишкеке</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T7</td>\n",
       "      <td>ORG 316 319</td>\n",
       "      <td>МВД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T8</td>\n",
       "      <td>GEOPOLIT 320 328</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>T9</td>\n",
       "      <td>GEOPOLIT 492 500</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T10</td>\n",
       "      <td>GEOPOLIT 525 528</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>T11</td>\n",
       "      <td>ORG 955 958</td>\n",
       "      <td>МВД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>T12</td>\n",
       "      <td>GEOPOLIT 1059 1062</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>T13</td>\n",
       "      <td>GEOPOLIT 1144 1147</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>T14</td>\n",
       "      <td>ORG 1804 1807</td>\n",
       "      <td>МВД</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>T15</td>\n",
       "      <td>LOC 1874 1881</td>\n",
       "      <td>Бишкека</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>T16</td>\n",
       "      <td>MEDIA 1951 1962</td>\n",
       "      <td>РИА Новости</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>T17</td>\n",
       "      <td>GEOPOLIT 1993 1996</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>T18</td>\n",
       "      <td>GEOPOLIT 2026 2034</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>T19</td>\n",
       "      <td>GEOPOLIT 2083 2091</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>T20</td>\n",
       "      <td>GEOPOLIT 2150 2153</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>T21</td>\n",
       "      <td>GEOPOLIT 2201 2209</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>T22</td>\n",
       "      <td>GEOPOLIT 2579 2582</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>T23</td>\n",
       "      <td>GEOPOLIT 2651 2654</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>T24</td>\n",
       "      <td>GEOPOLIT 2657 2665</td>\n",
       "      <td>Киргизия</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>T25</td>\n",
       "      <td>GEOPOLIT 2740 2748</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>T26</td>\n",
       "      <td>ORG 2833 2840</td>\n",
       "      <td>Манас</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>T27</td>\n",
       "      <td>GEOPOLIT 2849 2857</td>\n",
       "      <td>Киргизии</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>T28</td>\n",
       "      <td>GEOPOLIT 3005 3008</td>\n",
       "      <td>США</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>T29</td>\n",
       "      <td>MEDIA 3022 3033</td>\n",
       "      <td>РИА Новости</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0                   1               2\n",
       "0    T1           LOC 82 89         Бишкеке\n",
       "1    T2         LOC 113 119          БИШКЕК\n",
       "2    T3       MEDIA 132 146  Новости-Грузия\n",
       "3    T4    GEOPOLIT 175 183        Киргизии\n",
       "4    T5    GEOPOLIT 225 228             США\n",
       "5    T6         LOC 231 238         Бишкеке\n",
       "6    T7         ORG 316 319             МВД\n",
       "7    T8    GEOPOLIT 320 328        Киргизии\n",
       "8    T9    GEOPOLIT 492 500        Киргизии\n",
       "9   T10    GEOPOLIT 525 528             США\n",
       "10  T11         ORG 955 958             МВД\n",
       "11  T12  GEOPOLIT 1059 1062             США\n",
       "12  T13  GEOPOLIT 1144 1147             США\n",
       "13  T14       ORG 1804 1807             МВД\n",
       "14  T15       LOC 1874 1881         Бишкека\n",
       "15  T16     MEDIA 1951 1962     РИА Новости\n",
       "16  T17  GEOPOLIT 1993 1996             США\n",
       "17  T18  GEOPOLIT 2026 2034        Киргизии\n",
       "18  T19  GEOPOLIT 2083 2091        Киргизии\n",
       "19  T20  GEOPOLIT 2150 2153             США\n",
       "20  T21  GEOPOLIT 2201 2209        Киргизии\n",
       "21  T22  GEOPOLIT 2579 2582             США\n",
       "22  T23  GEOPOLIT 2651 2654             США\n",
       "23  T24  GEOPOLIT 2657 2665        Киргизия\n",
       "24  T25  GEOPOLIT 2740 2748        Киргизии\n",
       "25  T26       ORG 2833 2840           Манас\n",
       "26  T27  GEOPOLIT 2849 2857        Киргизии\n",
       "27  T28  GEOPOLIT 3005 3008             США\n",
       "28  T29     MEDIA 3022 3033     РИА Новости"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann = pd.read_csv('Collection5/003.ann', delimiter='\\t', header=None)\n",
    "ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "640b93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Составляем списки токенов и интенсов (из файла .ann делается словарь {слово : интенс}, из словаря каждому токену сопоствляем интенс)\n",
    "docs = []\n",
    "for i in range(len(documents_ann)):\n",
    "    words = []\n",
    "    labels = []\n",
    "    # Подготавливаем текст\n",
    "    text = data_text['text'][i]\n",
    "    \n",
    "    df = pd.read_csv('Collection5/' + documents_ann[i], delimiter='\\t', header=None)\n",
    "    df_ann = pd.DataFrame()\n",
    "    df_ann['Token'] = df.loc[:, 2]\n",
    "    split_1 = [loc.split() for loc in df.loc[:, 1].values]\n",
    "    df_ann['Entity'] = [loc[0] for loc in split_1]\n",
    "       \n",
    "    dic = {}\n",
    "    for j in range(len(df)):\n",
    "        token = df_ann['Token'][j].lower().split()\n",
    "        entity = df_ann['Entity'][j]\n",
    "        for tok in token:\n",
    "            dic[tok] = entity\n",
    "\n",
    "    for token in tokenize(text):\n",
    "        if (token.text.lower() in dic.keys()):\n",
    "            words.append(token.text)\n",
    "            labels.append(dic[token.text.lower()])\n",
    "        else:\n",
    "            words.append(token.text)\n",
    "            labels.append('OUT')\n",
    "    \n",
    "    docs.append([words, labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bf157e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Россия\tGEOPOLIT\n",
      "рассчитывает\tOUT\n",
      "на\tOUT\n",
      "конструктивное\tOUT\n",
      "воздействие\tOUT\n",
      "США\tGEOPOLIT\n",
      "на\tOUT\n",
      "Грузию\tGEOPOLIT\n",
      "04/08/2008\tOUT\n",
      "12\tOUT\n",
      ":\tOUT\n",
      "08\tOUT\n",
      "МОСКВА\tLOC\n",
      ",\tOUT\n",
      "4\tOUT\n",
      "авг\tOUT\n",
      "-\tOUT\n",
      "РИА\tMEDIA\n",
      "Новости\tMEDIA\n",
      ".\tOUT\n",
      "Россия\tGEOPOLIT\n",
      "рассчитывает\tOUT\n",
      ",\tOUT\n",
      "что\tOUT\n",
      "США\tGEOPOLIT\n",
      "воздействуют\tOUT\n",
      "на\tOUT\n",
      "Тбилиси\tGEOPOLIT\n",
      "в\tOUT\n",
      "связи\tOUT\n",
      "с\tOUT\n",
      "обострением\tOUT\n",
      "ситуации\tOUT\n",
      "в\tOUT\n",
      "зоне\tOUT\n",
      "грузино-осетинского\tOUT\n",
      "конфликта\tOUT\n",
      ".\tOUT\n",
      "Об\tOUT\n",
      "этом\tOUT\n",
      "статс-секретарь\tOUT\n",
      "-\tOUT\n",
      "заместитель\tOUT\n",
      "министра\tOUT\n",
      "иностранных\tOUT\n",
      "дел\tOUT\n",
      "России\tGEOPOLIT\n",
      "Григорий\tPER\n",
      "Карасин\tPER\n",
      "заявил\tOUT\n",
      "в\tOUT\n",
      "телефонном\tOUT\n",
      "разговоре\tOUT\n",
      "с\tOUT\n",
      "заместителем\tOUT\n",
      "госсекретаря\tOUT\n",
      "США\tGEOPOLIT\n",
      "Дэниэлом\tPER\n",
      "Фридом\tPER\n",
      ".\tOUT\n",
      "\"\tOUT\n",
      "С\tOUT\n",
      "российской\tOUT\n",
      "стороны\tOUT\n",
      "выражена\tOUT\n",
      "глубокая\tOUT\n",
      "озабоченность\tOUT\n",
      "в\tOUT\n",
      "связи\tOUT\n",
      "с\tOUT\n",
      "новым\tOUT\n",
      "витком\tOUT\n",
      "напряженности\tOUT\n",
      "вокруг\tOUT\n",
      "Южной\tGEOPOLIT\n",
      "Осетии\tGEOPOLIT\n",
      ",\tOUT\n",
      "противозаконными\tOUT\n",
      "действиями\tOUT\n",
      "грузинской\tOUT\n",
      "стороны\tOUT\n",
      "по\tOUT\n",
      "наращиванию\tOUT\n",
      "своих\tOUT\n",
      "вооруженных\tOUT\n",
      "сил\tOUT\n",
      "в\tOUT\n",
      "регионе\tOUT\n",
      ",\tOUT\n",
      "бесконтрольным\tOUT\n",
      "строительством\tOUT\n",
      "фортификационных\tOUT\n",
      "сооружений\tOUT\n",
      "\"\tOUT\n",
      ",\tOUT\n",
      "-\tOUT\n",
      "говорится\tOUT\n",
      "в\tOUT\n",
      "сообщении\tOUT\n",
      ".\tOUT\n",
      "\"\tOUT\n",
      "Россия\tGEOPOLIT\n",
      "уже\tOUT\n",
      "призвала\tOUT\n",
      "Тбилиси\tGEOPOLIT\n",
      "к\tOUT\n",
      "ответственной\tOUT\n",
      "линии\tOUT\n",
      "и\tOUT\n",
      "рассчитывает\tOUT\n",
      "также\tOUT\n",
      "на\tOUT\n",
      "конструктивное\tOUT\n",
      "воздействие\tOUT\n",
      "со\tOUT\n",
      "стороны\tOUT\n",
      "Вашингтона\tGEOPOLIT\n",
      "\"\tOUT\n",
      ",\tOUT\n",
      "-\tOUT\n",
      "сообщил\tOUT\n",
      "МИД\tORG\n",
      "России\tGEOPOLIT\n",
      ".\tOUT\n"
     ]
    }
   ],
   "source": [
    "data, labels = list(zip(*docs))\n",
    "for w, e in zip(data[0], labels[0]):\n",
    "    print(f'{w}\\t{e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1014a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_id</th>\n",
       "      <th>data</th>\n",
       "      <th>entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Россия</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>рассчитывает</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>на</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>конструктивное</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>воздействие</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>США</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>на</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Грузию</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>04/08/2008</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>:</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>08</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>МОСКВА</td>\n",
       "      <td>LOC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>авг</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>РИА</td>\n",
       "      <td>MEDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>Новости</td>\n",
       "      <td>MEDIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>Россия</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>рассчитывает</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>что</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>США</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>воздействуют</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>на</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>Тбилиси</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>в</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>связи</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>с</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>обострением</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>ситуации</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>в</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0</td>\n",
       "      <td>зоне</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>грузино-осетинского</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>конфликта</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0</td>\n",
       "      <td>.</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0</td>\n",
       "      <td>Об</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>этом</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>статс-секретарь</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0</td>\n",
       "      <td>заместитель</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0</td>\n",
       "      <td>министра</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>иностранных</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0</td>\n",
       "      <td>дел</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>России</td>\n",
       "      <td>GEOPOLIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0</td>\n",
       "      <td>Григорий</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>Карасин</td>\n",
       "      <td>PER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>заявил</td>\n",
       "      <td>OUT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_id                 data  entities\n",
       "0         0               Россия  GEOPOLIT\n",
       "1         0         рассчитывает       OUT\n",
       "2         0                   на       OUT\n",
       "3         0       конструктивное       OUT\n",
       "4         0          воздействие       OUT\n",
       "5         0                  США  GEOPOLIT\n",
       "6         0                   на       OUT\n",
       "7         0               Грузию  GEOPOLIT\n",
       "8         0           04/08/2008       OUT\n",
       "9         0                   12       OUT\n",
       "10        0                    :       OUT\n",
       "11        0                   08       OUT\n",
       "12        0               МОСКВА       LOC\n",
       "13        0                    ,       OUT\n",
       "14        0                    4       OUT\n",
       "15        0                  авг       OUT\n",
       "16        0                    -       OUT\n",
       "17        0                  РИА     MEDIA\n",
       "18        0              Новости     MEDIA\n",
       "19        0                    .       OUT\n",
       "20        0               Россия  GEOPOLIT\n",
       "21        0         рассчитывает       OUT\n",
       "22        0                    ,       OUT\n",
       "23        0                  что       OUT\n",
       "24        0                  США  GEOPOLIT\n",
       "25        0         воздействуют       OUT\n",
       "26        0                   на       OUT\n",
       "27        0              Тбилиси  GEOPOLIT\n",
       "28        0                    в       OUT\n",
       "29        0                связи       OUT\n",
       "30        0                    с       OUT\n",
       "31        0          обострением       OUT\n",
       "32        0             ситуации       OUT\n",
       "33        0                    в       OUT\n",
       "34        0                 зоне       OUT\n",
       "35        0  грузино-осетинского       OUT\n",
       "36        0            конфликта       OUT\n",
       "37        0                    .       OUT\n",
       "38        0                   Об       OUT\n",
       "39        0                 этом       OUT\n",
       "40        0      статс-секретарь       OUT\n",
       "41        0                    -       OUT\n",
       "42        0          заместитель       OUT\n",
       "43        0             министра       OUT\n",
       "44        0          иностранных       OUT\n",
       "45        0                  дел       OUT\n",
       "46        0               России  GEOPOLIT\n",
       "47        0             Григорий       PER\n",
       "48        0              Карасин       PER\n",
       "49        0               заявил       OUT"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'sent_id': [i for j in [[i] * len(s) for i, s in enumerate(data)] for i in j],\n",
    "                   'data': [i for j in data for i in j],\n",
    "                   'entities': [i for j in labels for i in j]})\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b5229ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection, preprocessing, linear_model\n",
    "\n",
    "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['data'], df['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47f5caed",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = preprocessing.LabelEncoder()\n",
    "train_y = encoder.fit_transform(train_y)\n",
    "valid_y = encoder.fit_transform(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59e864b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GEOPOLIT', 'LOC', 'MEDIA', 'ORG', 'OUT', 'PER'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d81abc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.apply(len).max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3da6278",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tensorflow.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "valid_data = tensorflow.data.Dataset.from_tensor_slices((valid_x, valid_y))\n",
    "\n",
    "train_data = train_data.batch(8)\n",
    "valid_data = valid_data.batch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "691bf4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tensorflow.data.experimental.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "valid_data = valid_data.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a3229cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    # Здесь может быть предобработка текста\n",
    "    return input_data\n",
    "\n",
    "vocab_size = 30000\n",
    "seq_len = 10\n",
    "\n",
    "vectorize_layer = TextVectorization(  \n",
    "                            standardize=custom_standardization,\n",
    "                            max_tokens=vocab_size,\n",
    "                            output_mode='int',\n",
    "                            #ngrams=(1, 3),\n",
    "                            output_sequence_length=seq_len)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "text_data = train_data.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4d31a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel = modelNER()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7534267c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mmodel.compile(optimizer='adam',\n",
    "              loss=tensorflow.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e0358f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "24843/24843 [==============================] - 466s 19ms/step - loss: 0.2681 - accuracy: 0.9229 - val_loss: 0.1932 - val_accuracy: 0.9435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6768\\3492764034.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m mmodel.fit( train_data,\n\u001b[0m\u001b[0;32m      2\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m             epochs=3)\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1547\u001b[0m             \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1549\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1550\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1551\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mmodel.fit(train_data, validation_data=valid_data, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe22350e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2071/2071 [==============================] - 9s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "pred_y = mmodel.predict(valid_x)\n",
    "y_pred_classes = np.argmax(pred_y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ade84888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.940405093291417"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1 = f1_score(valid_y, y_pred_classes, average= \"weighted\")\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac616c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
